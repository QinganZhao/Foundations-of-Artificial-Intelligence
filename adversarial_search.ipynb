{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 4700 - Homework 3: Adversarial Search\n",
    "<h3 style=\"color: red;\">\n",
    "    Due: Monday, March 4, 1:24pm on Gradescope\n",
    "    <br><br>\n",
    "    Late submission: Wednesday, March 6, 1:24pm on Gradescope (50% penalty)\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which other students did you interact with on this assignment? Provide their NetID(s) below consistent with the [homework policy](http://www.cs.cornell.edu/courses/cs4700/2019sp/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jd952, xl738"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written Question\n",
    "\n",
    "<h3 style=\"text-align: right; margin-top: -1em;\">25 points</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Consider the following game tree.  Each game state has two actions that can be taken: Left and Right.  The numbers at the terminal nodes are the values given by the evaluation function.\n",
    "  1. Which move should be made at the top level (Left or Right)?  What is its value?\n",
    "  1. Which nodes get pruned if you use alpha-beta pruning when you explore successors of states going left-to-right? \n",
    "  1. Give the answer to (B) when successors are explored right-to-left.\n",
    "  \n",
    "![](game_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1A:<br/>\n",
    "**Right** move should be made at the top level. The value is **5**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1B: <br/>\n",
    "**[S, K, AA, O]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1C:<br/>\n",
    "**[D]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Consider using minimax search with alpha–beta pruning on a game whose branching factor is $b$ and where you search to depth $m$. If successor nodes are searched in the worst-case order no pruning happens and the algorithm takes $O(b^m)$ time.  (Make sure you understand why that is true.) It has been shown that if successors are ordered in the best-case optimal fashion the algorithm’s time complexity will be $O(b^{\\frac{m}{2}})$. This means the best-case scenario will take less time to search to the same depth $m$. Let’s say the worst-case ordering of successors runs for time $T$.  How much deeper can you search in the best-case scenario if it is also allowed to run for time $T$?  To make this as simple as possible, assume the constants that are “hidden” in both big-O time complexity expressions are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is **2m**.<br /><br />\n",
    "In the worst case, searching depth *$m$* will result in *$O(b^m)$* time. We denote the searching depth by $d$ where we also take $T$ (i.e., $O(b^m)$) time in the best case. We then get $O(b^{\\frac{d}{2}})=T=O(b^m)$. Hence, $d=2m$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the assignment will provide you with programming exercises to help cement the key ideas about adversarial search. For this purpose, we have chosen a simple 2-player strategy game called __Neutron__ as our domain of interest. It is played on a 5 x 5 board. Each player has 5 soldier pieces and a respective home row. The soldiers for each player start by occupying their home row; for player 1, this is row 1, and for player 2, this is row 5. In addition, there is a neutral piece called the Neutron located in the center of the board that both players control.\n",
    "\n",
    "For our purposes, we have chosen to make each piece distinct from one another. The soldier pieces for player 1 are marked by the first 5 letters of the alphabet. The soldier pieces for player 2 are marked by the first 5 positive integers. The Neutron piece is marked by a \\*. The initial position of the board (as well row and column numbers) can be illustrated as:\n",
    "\n",
    "```\n",
    "          1   2   3   4   5\n",
    "\n",
    "        ---------------------\n",
    "1       | A | B | C | D | E |\n",
    "        ---------------------\n",
    "2       |   |   |   |   |   |\n",
    "        ---------------------\n",
    "3       |   |   | * |   |   |\n",
    "        ---------------------\n",
    "4       |   |   |   |   |   |\n",
    "        ---------------------\n",
    "5       | 1 | 2 | 3 | 4 | 5 |\n",
    "        ---------------------\n",
    "```\n",
    "\n",
    "__RULES:__ The game begins with player 1 and then alternates between players. In the first turn, player 1 must select 1 of their own soldier pieces to move. Then, for every turn after this, a player must make 2 sequential moves. First, they must move the Neutron piece. Second, they must select 1 of their own soldier pieces to move. The movement of a piece (soldier or Neutron) can be in either of the 4 directions: up, left, down, right. A piece can move in a direction as long as the path is clear. A piece cannot take another piece, jump over a piece, or otherwise change directions. They must move as many spaces as possible in the direction chosen until they collide with another piece or a wall.\n",
    "\n",
    "For example, let us use the initial state illustrated above. Since this is the beginning of the game, player 1 has 5 available choices. Note, coordinates are in (row,column) format.\n",
    "\n",
    "1. Move A from (1,1) down to (4,1)\n",
    "1. Move B from (1,2) down to (4,2)\n",
    "1. Move C from (1,3) down to (2,3)\n",
    "1. Move D from (1,4) down to (4,4)\n",
    "1. Move E from (1,5) down to (4,5)\n",
    "\n",
    "Let's say player 1 decides to use move 4 from the list above. The resulting state would be:\n",
    "\n",
    "```\n",
    "          1   2   3   4   5\n",
    "\n",
    "        ---------------------\n",
    "1       | A | B | C |   | E |\n",
    "        ---------------------\n",
    "2       |   |   |   |   |   |\n",
    "        ---------------------\n",
    "3       |   |   | * |   |   |\n",
    "        ---------------------\n",
    "4       |   |   |   | D |   |\n",
    "        ---------------------\n",
    "5       | 1 | 2 | 3 | 4 | 5 |\n",
    "        ---------------------\n",
    "```\n",
    "\n",
    "It is now player 2's turn. Since this is no longer the initial state of the game, the player must make 2 sequential moves (Neutron followed by a soldier). Therefore, player 2 has 15 possible actions to choose from:\n",
    "\n",
    "1. Move * from (3,3) up to (2,3), then move 1 from (5,1) up to (2,1)\n",
    "1. Move * from (3,3) up to (2,3), then move 2 from (5,2) up to (2,2)\n",
    "1. Move * from (3,3) up to (2,3), then move 3 from (5,3) up to (3,3)\n",
    "1. Move * from (3,3) up to (2,3), then move 5 from (5,5) up to (2,5)\n",
    "1. Move * from (3,3) left to (3,1), then move 1 from (5,1) up to (4,1)\n",
    "1. Move * from (3,3) left to (3,1), then move 2 from (5,2) up to (2,2)\n",
    "1. Move * from (3,3) left to (3,1), then move 3 from (5,3) up to (2,3)\n",
    "1. Move * from (3,3) left to (3,1), then move 5 from (5,5) up to (2,5)\n",
    "1. Move * from (3,3) down to (4,3), then move 1 from (5,1) up to (2,1)\n",
    "1. Move * from (3,3) down to (4,3), then move 2 from (5,2) up to (2,2)\n",
    "1. Move * from (3,3) down to (4,3), then move 5 from (5,5) up to (2,5)\n",
    "1. Move * from (3,3) right to (3,5), then move 1 from (5,1) up to (2,1)\n",
    "1. Move * from (3,3) right to (3,5), then move 2 from (5,2) up to (2,2)\n",
    "1. Move * from (3,3) right to (3,5), then move 3 from (5,3) up to (2,3)\n",
    "1. Move * from (3,3) right to (3,5), then move 5 from (5,5) up to (4,5)\n",
    "\n",
    "This back-and-forth will continue until the game ends. So, how does the game of Neutron end?\n",
    "\n",
    "__GOALS:__ The main goal of the game is for a player to move the Neutron piece to their home row. This can happen in a couple of ways. Either the player is able to move the Neutron on their own turn to their home row, or can otherwise force the opponent into moving the Neutron to the player's home row. Alternatively, it is also possible to win the game without securing the Neutron by blocking your opponent from moving, i.e. when your opponent is deadlocked, you also win.\n",
    "\n",
    "Run the code cell below which contains the logic for __Neutron__. Feel free to look through the code for a better understanding of the game implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "# additional definitions\n",
    "GameState = namedtuple('GameState', 'board, player, is_initial, utility_value')\n",
    "PLAYER_1  = ['A', 'B', 'C', 'D', 'E']\n",
    "PLAYER_2  = ['1', '2', '3', '4', '5']\n",
    "NEUTRON   = '*'\n",
    "BLANK     = ' '\n",
    "\n",
    "\"\"\"\n",
    "The Neutron game. Implemented as a class so that multiple instances can be created.\n",
    "\n",
    "A state for this game is a 4-tuple, (board, player, is_initial, utility_value).\n",
    "- board, a dictionary with (i,j) coordinates as the key, and the piece as\n",
    "  the value. Valid coordinate values for i and j are 1 to 5, inclusive. The piece\n",
    "  should refer to one of the variables defined above.\n",
    "- player, refers to the player that needs to move in this state. Should reference\n",
    "  one of the variables defined above.\n",
    "- is_initial, a Boolean value that determines whether or not this state is the\n",
    "  initial state of the game.\n",
    "- utility_value, an integer value that represents the utility value for the\n",
    "  state.\n",
    "\"\"\"\n",
    "class NeutronGame:\n",
    "    \"\"\"\n",
    "    Constructor for the class. Initializes the initial state of the game, the move\n",
    "    ordering that should be used, and the number of states generated during a game.\n",
    "    The game starts with all of player 1's pieces at row 1, all of player 2's pieces\n",
    "    at row 5, and the neutron in the center. The fixed move ordering is up, down,\n",
    "    left, then right.\n",
    "    \n",
    "    Optional parameters:\n",
    "    - custom_state, a user-provided state to use as the initial state of the game\n",
    "    - move_order, a custom move order to be used in actions()\n",
    "    \"\"\"\n",
    "    def __init__(self, custom_state = None, move_order = None):\n",
    "        if custom_state:\n",
    "            self.initial_state = custom_state\n",
    "        else:\n",
    "            board = {(1, j + 1) : PLAYER_1[j] for j in range(5)}\n",
    "            board.update({(i,j) : BLANK for j in range(1,6) for i in range(2,5)})\n",
    "            board.update({(5, j + 1) : PLAYER_2[j] for j in range(5)})\n",
    "            board[(3,3)] = NEUTRON\n",
    "            self.initial_state = GameState(board = board, player = PLAYER_1,\n",
    "                is_initial = True, utility_value = 0)\n",
    "        \n",
    "        if move_order:\n",
    "            self.move_order = move_order\n",
    "        else:\n",
    "            self.move_order = [(-1,0), (1,0), (0,-1), (0,1)]\n",
    "        \n",
    "        self.num_states_gen = 0\n",
    "\n",
    "    \"\"\"\n",
    "    Given a well-defined state as input, this function will return all the\n",
    "    possible actions that can be taken as a list. An action can consist of\n",
    "    1 or 2 moves, and so is treated as a list. A move is represented as a\n",
    "    tuple (piece, description), where piece refers to one of the global\n",
    "    variables and description is a string describing the movement of the piece.\n",
    "    \"\"\"\n",
    "    def actions(self, state):\n",
    "        acts = []\n",
    "\n",
    "        # must move the neutron piece first if this is not the initial state\n",
    "        neutron_moves = []\n",
    "        if not state.is_initial:\n",
    "            neutron_moves = [self.try_move(state.board, NEUTRON, i, j) \\\n",
    "                             for (i,j) in self.move_order]\n",
    "            neutron_moves = filter(lambda m : m != None, neutron_moves)\n",
    "            neutron_moves = map(lambda m : self.simplify_move(state.board, m),\n",
    "                neutron_moves)\n",
    "\n",
    "        # if the neutron was moved, must temporarily transition the state first\n",
    "        # then move the soldier pieces\n",
    "        for nm in neutron_moves:\n",
    "            temp_board = self.result(state, [nm]).board\n",
    "            for piece in state.player:\n",
    "                moves = [self.try_move(temp_board, piece, i, j) \\\n",
    "                         for (i,j) in self.move_order]\n",
    "                moves = filter(lambda m : m != None, moves)\n",
    "                moves = map(lambda m : self.simplify_move(temp_board, m), moves)\n",
    "                for m in moves:\n",
    "                    acts.append([nm, m])\n",
    "\n",
    "        # otherwise, just move the soldier pieces for the player\n",
    "        if not neutron_moves:\n",
    "            for piece in state.player:\n",
    "                moves = [self.try_move(state.board, piece, i, j) \\\n",
    "                         for (i,j) in self.move_order]\n",
    "                moves = filter(lambda m : m != None, moves)\n",
    "                moves = map(lambda m : self.simplify_move(state.board, m),\n",
    "                    moves)\n",
    "                for m in moves:\n",
    "                    acts.append([m])\n",
    "\n",
    "        return acts\n",
    "\n",
    "    \"\"\"\n",
    "    Try moving the piece on the board by shifting the piece's coordinates\n",
    "    by the amount (delta_i,delta_j). Return the resulting move.\n",
    "    \"\"\"\n",
    "    def try_move(self, board, piece, delta_i, delta_j):\n",
    "        move   = None\n",
    "        (i,j)  = self.find_piece(board, piece)\n",
    "        temp_i = i + delta_i\n",
    "        temp_j = j + delta_j\n",
    "        while temp_i >= 1 and temp_i <= 5 and temp_j >= 1 and temp_j <= 5 \\\n",
    "        and board[(temp_i,temp_j)] == BLANK:\n",
    "            temp_i += delta_i\n",
    "            temp_j += delta_j\n",
    "        temp_i -= delta_i\n",
    "        temp_j -= delta_j\n",
    "        if board[(temp_i,temp_j)] == BLANK:\n",
    "            move = (piece, (temp_i,temp_j))\n",
    "        return move\n",
    "\n",
    "    \"\"\"\n",
    "    Find the piece on the board and return its coordinates.\n",
    "    \"\"\"\n",
    "    def find_piece(self, board, piece):\n",
    "        i, j = None, None\n",
    "        for (m,n) in iter(board):\n",
    "            if board[(m,n)] == piece:\n",
    "                i, j = m, n\n",
    "                break\n",
    "        return (i,j)\n",
    "\n",
    "    \"\"\"\n",
    "    Given move as the tuple (piece, description), convert the description into\n",
    "    delta values to try moving the piece on the given board. Return the\n",
    "    resulting move.\n",
    "    \"\"\"\n",
    "    def decipher_move(self, board, move):\n",
    "        (piece,desc) = move\n",
    "        (i,j)        = self.find_piece(board, piece)\n",
    "        vert         = {'UP' : -1, 'DOWN' : 1}\n",
    "        horiz        = {'LEFT' : -1, 'RIGHT' : 1}\n",
    "        return self.try_move(board, piece, vert.get(desc,0), horiz.get(desc,0))\n",
    "        \n",
    "    \"\"\"\n",
    "    Given a move in the format (piece, (i,j)), convert the movement into a simple\n",
    "    description that is user-friendly.\n",
    "    \"\"\"\n",
    "    def simplify_move(self, board, move):\n",
    "        (piece, (i,j)) = move\n",
    "        (m,n)          = self.find_piece(board, piece)\n",
    "        if i < m:\n",
    "            desc = 'UP'\n",
    "        elif i > m:\n",
    "            desc = 'DOWN'\n",
    "        elif j < n:\n",
    "            desc = 'LEFT'\n",
    "        else:\n",
    "            desc = 'RIGHT'\n",
    "        return (piece,desc)\n",
    "\n",
    "    \"\"\"\n",
    "    Given a state and an action, return the new state that our game should\n",
    "    transition to. Assumes state and action are valid.\n",
    "    \"\"\"\n",
    "    def result(self, state, action):\n",
    "        board                = state.board.copy()\n",
    "        player               = state.player\n",
    "        self.num_states_gen += 1\n",
    "        if not action:\n",
    "            return GameState(board = board, is_initial = False, player = player,\n",
    "                utility_value = 1 if state.player == PLAYER_2 else -1)\n",
    "        for a in action:\n",
    "            (piece, (i,j)) = self.decipher_move(board, a)\n",
    "            (m,n)          = self.find_piece(board, piece)\n",
    "            board[(m,n)]   = BLANK\n",
    "            board[(i,j)]   = piece\n",
    "        return GameState(board = board, is_initial = False,\n",
    "            player = PLAYER_2 if player == PLAYER_1 else PLAYER_1,\n",
    "            utility_value = self.compute_utility(board))\n",
    "\n",
    "    \"\"\"\n",
    "    Return the utility of the given state from the perspective of\n",
    "    the given player.\n",
    "    \"\"\"\n",
    "    def utility(self, state, player):\n",
    "        u = state.utility_value\n",
    "        if u == 0 and not self.actions(state):\n",
    "            return -1 if state.player == player else 1\n",
    "        return u if player == PLAYER_1 else -u\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the utility of the board from the perspective of player 1.\n",
    "    +1 if the neutron is in row 1, -1 if the neutron is in row 5,\n",
    "    0 otherwise.\n",
    "    \"\"\"\n",
    "    def compute_utility(self, board):\n",
    "        (i,j) = self.find_piece(board, NEUTRON)\n",
    "        u     = 0\n",
    "        if i == 1:\n",
    "            u = 1\n",
    "        elif i == 5:\n",
    "            u = -1\n",
    "        return u\n",
    "\n",
    "    \"\"\"\n",
    "    A state is terminal if the utility value is non-zero or if\n",
    "    there are no available actions to take in that state.\n",
    "    \"\"\"\n",
    "    def terminal_test(self, state):\n",
    "        return state.utility_value != 0 or not self.actions(state)\n",
    "\n",
    "    \"\"\"\n",
    "    Print the state to console.\n",
    "    \"\"\"\n",
    "    def display(self, state):\n",
    "        board_size = 5\n",
    "        s          = '\\n'\n",
    "        for _ in range(board_size * 4 + 1):\n",
    "            s += '-'\n",
    "        print(s)\n",
    "\n",
    "        for i in range(1, board_size + 1):\n",
    "            s = ''\n",
    "            for j in range(1, board_size + 1):\n",
    "                s += '| ' + state.board[(i,j)] + ' '\n",
    "            s += '|\\n'\n",
    "            for _ in range(board_size * 4 + 1):\n",
    "                s += '-'\n",
    "            print(s)\n",
    "\n",
    "        print('Player to move:',\n",
    "              'Player 1' if state.player == PLAYER_1 else 'Player 2')\n",
    "\n",
    "    \"\"\"\n",
    "    Let two agents play the game from the initial state. display_flag\n",
    "    toggles whether or not to show every state during gameplay.\n",
    "    experiment_flag will suppress all output.\n",
    "    \n",
    "    These are the 4 possible outputs:\n",
    "    (1,1) means player 2 wins, caused by player 1\n",
    "    (1,2) means player 1 wins, caused by player 2\n",
    "    (-1,1) means player 1 wins, caused by player 1\n",
    "    (-1,2) means player 2 wins, caused by player 2\n",
    "    \"\"\"\n",
    "    def play_game(self, first_player, second_player, display_flag, experiment_flag = False):\n",
    "        state = self.initial_state\n",
    "        if display_flag:\n",
    "            self.display(state)\n",
    "        while True:\n",
    "            for player in [first_player, second_player]:\n",
    "                action = player.play_turn(self, state)\n",
    "                state  = self.result(state, action)\n",
    "                if display_flag:\n",
    "                    self.display(state)\n",
    "                if self.terminal_test(state):\n",
    "                    if not experiment_flag:\n",
    "                        print('\\nGame is over!')\n",
    "                        self.display(state)\n",
    "                    return (self.utility(state, state.player),\n",
    "                            1 if player == first_player else 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's introduce 2 types of agents that will be able to play our game. The first, called `QueryAgent`, acts as a stand-in for the user. It will output the possible actions that the user can take and ask for input on which action to undergo. The second, called `RandomAgent`, simply chooses amongst the available actions at random. Note that an action in our implementation is a list of tuples (wherein a tuple represents a single move). Therefore, the input you provide the agent should also be a list of tuples.\n",
    "\n",
    "For example, reusing our example from the introduction, the actions for player 1 at the initial state would be displayed as:\n",
    "\n",
    "`[[('A', 'DOWN')], [('B', 'DOWN')], [('C', 'DOWN')], [('D', 'DOWN')], [('E', 'DOWN')]]`\n",
    "\n",
    "Suppose player 1 decides to move its soldier piece B down. Then, as input, it would supply: `[('B', 'DOWN')]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An agent that relies on user input.\n",
    "\"\"\"\n",
    "class QueryAgent:\n",
    "    def play_turn(self, game, state):\n",
    "        action = None\n",
    "        print('\\nAvailable actions:', game.actions(state))\n",
    "        if game.actions(state):\n",
    "            action_str = input('\\nYour action: ')\n",
    "            try:\n",
    "                action = eval(action_str)\n",
    "            except NameError:\n",
    "                action = action_str\n",
    "        else:\n",
    "            print('No legal actions remain!')\n",
    "        return action\n",
    "\n",
    "\"\"\"\n",
    "An agent that will randomly choose amongst the available actions.\n",
    "\"\"\"\n",
    "class RandomAgent:\n",
    "    def play_turn(self, game, state):\n",
    "        return random.choice(game.actions(state)) if game.actions(state) else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a game and agents that can play it. To ensure you have a proper understanding of __Neutron__, let's have you play against a random agent. Run the code cell below to try your best to win!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------\n",
      "| A | B | C | D | E |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "|   |   | * |   |   |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "| 1 | 2 | 3 | 4 | 5 |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "Available actions: [[('A', 'DOWN')], [('B', 'DOWN')], [('C', 'DOWN')], [('D', 'DOWN')], [('E', 'DOWN')]]\n",
      "\n",
      "Your action: [('B', 'DOWN')]\n",
      "\n",
      "---------------------\n",
      "| A |   | C | D | E |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "|   |   | * |   |   |\n",
      "---------------------\n",
      "|   | B |   |   |   |\n",
      "---------------------\n",
      "| 1 | 2 | 3 | 4 | 5 |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "\n",
      "---------------------\n",
      "| A |   | C | D | E |\n",
      "---------------------\n",
      "|   |   | 3 |   |   |\n",
      "---------------------\n",
      "|   |   |   |   | * |\n",
      "---------------------\n",
      "|   | B |   |   |   |\n",
      "---------------------\n",
      "| 1 | 2 |   | 4 | 5 |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "Available actions: [[('*', 'UP'), ('A', 'DOWN')], [('*', 'UP'), ('A', 'RIGHT')], [('*', 'UP'), ('B', 'UP')], [('*', 'UP'), ('B', 'LEFT')], [('*', 'UP'), ('B', 'RIGHT')], [('*', 'UP'), ('C', 'LEFT')], [('*', 'UP'), ('D', 'DOWN')], [('*', 'DOWN'), ('A', 'DOWN')], [('*', 'DOWN'), ('A', 'RIGHT')], [('*', 'DOWN'), ('B', 'UP')], [('*', 'DOWN'), ('B', 'LEFT')], [('*', 'DOWN'), ('B', 'RIGHT')], [('*', 'DOWN'), ('C', 'LEFT')], [('*', 'DOWN'), ('D', 'DOWN')], [('*', 'DOWN'), ('E', 'DOWN')], [('*', 'LEFT'), ('A', 'DOWN')], [('*', 'LEFT'), ('A', 'RIGHT')], [('*', 'LEFT'), ('B', 'UP')], [('*', 'LEFT'), ('B', 'LEFT')], [('*', 'LEFT'), ('B', 'RIGHT')], [('*', 'LEFT'), ('C', 'LEFT')], [('*', 'LEFT'), ('D', 'DOWN')], [('*', 'LEFT'), ('E', 'DOWN')]]\n",
      "\n",
      "Your action: [('*', 'LEFT'), ('A', 'RIGHT')]\n",
      "\n",
      "---------------------\n",
      "|   | A | C | D | E |\n",
      "---------------------\n",
      "|   |   | 3 |   |   |\n",
      "---------------------\n",
      "| * |   |   |   |   |\n",
      "---------------------\n",
      "|   | B |   |   |   |\n",
      "---------------------\n",
      "| 1 | 2 |   | 4 | 5 |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "\n",
      "---------------------\n",
      "| * | A | C | D | E |\n",
      "---------------------\n",
      "|   |   | 3 |   |   |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "|   | B |   |   |   |\n",
      "---------------------\n",
      "| 1 |   | 2 | 4 | 5 |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "Game is over!\n",
      "\n",
      "---------------------\n",
      "| * | A | C | D | E |\n",
      "---------------------\n",
      "|   |   | 3 |   |   |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "|   | B |   |   |   |\n",
      "---------------------\n",
      "| 1 |   | 2 | 4 | 5 |\n",
      "---------------------\n",
      "Player to move: Player 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = NeutronGame()\n",
    "p1   = QueryAgent()\n",
    "p2   = RandomAgent()\n",
    "game.play_game(p1, p2, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, just for fun, let's have 2 random agents go at it. Toggle the display flag (third parameter for `play_game`) to see the full sequence of states for the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Game is over!\n",
      "\n",
      "---------------------\n",
      "|   | B | C |   |   |\n",
      "---------------------\n",
      "|   |   | 1 |   |   |\n",
      "---------------------\n",
      "| 5 |   |   |   | 4 |\n",
      "---------------------\n",
      "| 2 |   |   |   | E |\n",
      "---------------------\n",
      "| A | 3 | * |   | D |\n",
      "---------------------\n",
      "Player to move: Player 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = NeutronGame()\n",
    "p1   = RandomAgent()\n",
    "p2   = RandomAgent()\n",
    "game.play_game(p1, p2, False) # can toggle display flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Minimax and Maximin\n",
    "\n",
    "<h3 style=\"text-align: right; margin-top: -1em;\">20 points</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you learned in lecture, the __minimax__ and __maximin__ search algorithms provide a way for an agent to search an adversarial search space. Specifically, player 1 (traditionally called _MAX_) wants to maximize their score while player 2 (similarly referred to as _MIN_) wishes to minimize such a score. If we envision the search as a tree, then each depth corresponds to different responses (actions) that the current player can make to the opponent's previous action. As we descend this tree, the current player alternates until the search reaches a terminal state.\n",
    "\n",
    "We can summarize this procedure as the following:\n",
    "\n",
    "$$\n",
    "\\mbox{Minimax}(s) =\n",
    "\\begin{cases} \n",
    "      \\mbox{Utility}(s) & \\mbox{if Terminal}(s) \\\\\n",
    "      argmax_{a\\ \\in \\ \\mbox{Actions}(s)} \\mbox{Maximin(Result}(s, a)) & \\mbox{if Player}(s) = \\mbox{MAX} \\\\\n",
    "      argmin_{a\\ \\in \\ \\mbox{Actions}(s)} \\mbox{Minimax(Result}(s, a)) & \\mbox{if Player}(s) = \\mbox{MIN}\n",
    "   \\end{cases}\n",
    "$$\n",
    "\n",
    "Your first task in this assignment is to implement the pair of search algorithms. For easy reference, a modified pseudocode from lecture has been reproduced below. Note that, for this assigment, our algorithms must output two things: __a value and an action__. One handy feature about Python is that functions are allowed to return 2 outputs, so this works out. You can do this by using the syntax: `return val_1, val_2`. Lastly, remember that you __must__ also provide documentation for your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "minimax(game, state, cutoff, depth, eval_fn)\n",
    "    if cutoff(state, depth) then\n",
    "        return eval_fn(state, player 1), empty_action\n",
    "    else\n",
    "        val = -∞\n",
    "        best_act = empty_action\n",
    "        foreach act in actions(state)\n",
    "            val' = maximin(game, result(state, act), cutoff, depth + 1, eval_fn)\n",
    "            if val' > val then\n",
    "                val = val'\n",
    "                best_act = act\n",
    "    return val, best_act\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "maximin(game, state, cutoff, depth, eval_fn)\n",
    "    if cutoff(state, depth) then\n",
    "        return eval_fn(state, player 2), empty_action\n",
    "    else\n",
    "        val = ∞\n",
    "        best_act = empty_action\n",
    "        foreach act in actions(state)\n",
    "            val' = minimax(game, result(state, act), cutoff, depth + 1, eval_fn)\n",
    "            if val' < val then\n",
    "                val = val'\n",
    "                best_act = act\n",
    "    return val, best_act\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    ":Minimax search algorithm implentation\n",
    ":type game: object (an object generated by NeutronGame class)\n",
    ":type state: list (possible sates)\n",
    ":type cutoff: function (optional, customized cutoff function, default by game.terminal_test)\n",
    ":type depth: int (optional, recursion depth, default by 0)\n",
    ":type eval_fn: function (optional, default by game.utility)\n",
    ":rtype: tuple (returns a value and an action)\n",
    "\"\"\"\n",
    "def minimax(game, state, cutoff = None, depth = 0, eval_fn = None):\n",
    "    # default cutoff is just a test if the state is terminal\n",
    "    if cutoff == None:\n",
    "        cutoff = lambda s, d : game.terminal_test(s)\n",
    "    \n",
    "    # default eval is just the utility value\n",
    "    if eval_fn == None:\n",
    "        eval_fn = game.utility\n",
    "    \n",
    "    # replace the line below with your code\n",
    "    if cutoff(state, depth):\n",
    "        return eval_fn(state, p1), []\n",
    "    else:\n",
    "        val = float('-inf')\n",
    "        best_act = []\n",
    "        for act in game.actions(state):\n",
    "            val_curr = maximin(game, game.result(state, act), cutoff, depth+1, eval_fn)[0]\n",
    "            if val_curr > val:\n",
    "                val = val_curr\n",
    "                best_act = act\n",
    "    return val, best_act\n",
    "\n",
    "\"\"\"\n",
    ":Maximin search algorithm implentation\n",
    ":type game: object (an object generated by NeutronGame class)\n",
    ":type state: list (possible sates)\n",
    ":type cutoff: function (optional, customized cutoff function, default by game.terminal_test)\n",
    ":type depth: int (optional, recursion depth, default by 0)\n",
    ":type eval_fn: function (optional, default by game.utility)\n",
    ":rtype: tuple (returns a value and an action)\n",
    "\"\"\"\n",
    "def maximin(game, state, cutoff = None, depth = 0, eval_fn = None):\n",
    "    # default cutoff is just a test if the state is terminal\n",
    "    if cutoff == None:\n",
    "        cutoff = lambda s, d : game.terminal_test(s)\n",
    "    \n",
    "    # default eval is just the utility value\n",
    "    if eval_fn == None:\n",
    "        eval_fn = game.utility\n",
    "    \n",
    "    # replace the line below with your code\n",
    "    if cutoff(state, depth):\n",
    "        return eval_fn(state, p2), []\n",
    "    else:\n",
    "        val = float('inf')\n",
    "        best_act = []\n",
    "        for act in game.actions(state):\n",
    "            val_curr = minimax(game, game.result(state, act), cutoff, depth+1, eval_fn)[0]\n",
    "        if val_curr < val:\n",
    "            val = val_curr\n",
    "            best_act = act\n",
    "    return val, best_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a working implementation of __minimax__ and __maximin__, we can create agents based around these algorithms. Both agents can take optional parameters but we will focus on these parameters later in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An agent that utilizes the minimax search algorithm. Can take two optional parameters.\n",
    "cutoff, that can be used to determine when to stop the search\n",
    "eval_fn, which evaluates a state's value from the perspective of a given player\n",
    "\"\"\"\n",
    "class MinimaxAgent:\n",
    "    def __init__(self, cutoff = None, eval_fn = None):\n",
    "        self.cutoff  = cutoff\n",
    "        self.eval_fn = eval_fn\n",
    "\n",
    "    def play_turn(self, game, state):\n",
    "        _, best_act = minimax(game, state, cutoff = self.cutoff, eval_fn = self.eval_fn)\n",
    "        return best_act\n",
    "\n",
    "\"\"\"\n",
    "An agent that utilizes the maximin search algorithm. Can take two optional parameters.\n",
    "cutoff, that can be used to determine when to stop the search\n",
    "eval_fn, which evaluates a state's value from the perspective of a given player\n",
    "\"\"\"\n",
    "class MaximinAgent:\n",
    "    def __init__(self, cutoff = None, eval_fn = None):\n",
    "        self.cutoff  = cutoff\n",
    "        self.eval_fn = eval_fn\n",
    "\n",
    "    def play_turn(self, game, state):\n",
    "        _, best_act = maximin(game, state, cutoff = self.cutoff, eval_fn = self.eval_fn)\n",
    "        return best_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a test, we are going to start from a particular state partway through a game. This particular state was written so that the minimax agent will be able to playout until the end. Run the code cell below to check if your implementation is able to win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------\n",
      "| A |   | B | C | D |\n",
      "---------------------\n",
      "| 1 | * | 2 |   |   |\n",
      "---------------------\n",
      "|   | E |   |   |   |\n",
      "---------------------\n",
      "|   |   |   |   | 3 |\n",
      "---------------------\n",
      "|   | 4 |   | 5 |   |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "---------------------\n",
      "| A | * | B |   | D |\n",
      "---------------------\n",
      "| 1 |   | 2 |   |   |\n",
      "---------------------\n",
      "|   | E |   |   |   |\n",
      "---------------------\n",
      "|   |   |   | C | 3 |\n",
      "---------------------\n",
      "|   | 4 |   | 5 |   |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "\n",
      "Game is over!\n",
      "\n",
      "---------------------\n",
      "| A | * | B |   | D |\n",
      "---------------------\n",
      "| 1 |   | 2 |   |   |\n",
      "---------------------\n",
      "|   | E |   |   |   |\n",
      "---------------------\n",
      "|   |   |   | C | 3 |\n",
      "---------------------\n",
      "|   | 4 |   | 5 |   |\n",
      "---------------------\n",
      "Player to move: Player 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_board = {\n",
    "    (1,1) : PLAYER_1[0], (1,2) : BLANK, (1,3) : PLAYER_1[1], (1,4) : PLAYER_1[2], (1,5) : PLAYER_1[3],\n",
    "    (2,1) : PLAYER_2[0], (2,2) : NEUTRON, (2,3) : PLAYER_2[1], (2,4) : BLANK, (2,5) : BLANK,\n",
    "    (3,1) : BLANK, (3,2) : PLAYER_1[4], (3,3) : BLANK, (3,4) : BLANK, (3,5) : BLANK,\n",
    "    (4,1) : BLANK, (4,2) : BLANK, (4,3) : BLANK, (4,4) : BLANK, (4,5) : PLAYER_2[2],\n",
    "    (5,1) : BLANK, (5,2) : PLAYER_2[3], (5,3) : BLANK, (5,4) : PLAYER_2[4], (5,5) : BLANK,\n",
    "}\n",
    "custom_state = GameState(board = custom_board, player = PLAYER_1, is_initial = False, utility_value = 0)\n",
    "game         = NeutronGame(custom_state)\n",
    "p1           = MinimaxAgent()\n",
    "p2           = RandomAgent()\n",
    "game.play_game(p1, p2, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to have our minimax agent play against a random agent starting from the beginning of the game. When you run the code cell below, you will find that our minimax agent is unable to process the state space search! In fact, __your Python kernel will crash__, because it exceeds the maximum recursion depth. The takeaway from this is that, unfortunately, a pure minimax approach is intractable for our __Neutron__ domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-fdea8c4cce7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mp1\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mMinimaxAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mp2\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mRandomAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-fc5ddaece2d2>\u001b[0m in \u001b[0;36mplay_game\u001b[1;34m(self, first_player, second_player, display_flag, experiment_flag)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mplayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfirst_player\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecond_player\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay_turn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m                 \u001b[0mstate\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdisplay_flag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-286036524e98>\u001b[0m in \u001b[0;36mplay_turn\u001b[1;34m(self, game, state)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mplay_turn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_act\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminimax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcutoff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbest_act\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-b7b9caacd130>\u001b[0m in \u001b[0;36mminimax\u001b[1;34m(game, state, cutoff, depth, eval_fn)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mbest_act\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mact\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mval_curr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaximin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mval_curr\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_curr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-b7b9caacd130>\u001b[0m in \u001b[0;36mmaximin\u001b[1;34m(game, state, cutoff, depth, eval_fn)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mbest_act\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mact\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0mval_curr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminimax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mval_curr\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_curr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "... last 2 frames repeated, from the frame below ...\n",
      "\u001b[1;32m<ipython-input-5-b7b9caacd130>\u001b[0m in \u001b[0;36mminimax\u001b[1;34m(game, state, cutoff, depth, eval_fn)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mbest_act\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mact\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mval_curr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaximin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mval_curr\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_curr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "game = NeutronGame()\n",
    "p1   = MinimaxAgent()\n",
    "p2   = RandomAgent()\n",
    "game.play_game(p1, p2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Alpha-Beta Pruning\n",
    "\n",
    "<h3 style=\"text-align: right; margin-top: -1em;\">20 points</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw from part 1, while the minimax algorithm is a good start, this approach fails in our domain, due to the exponential number of states that it needs to explore. Recall from lecture that it is possible for us to improve the algorithm through the use of a pruning technique, called __alpha-beta pruning__. In particular, we use $\\alpha$ to refer to the value of the best choice (i.e. highest-value) so far for MAX, while $\\beta$ refers to the value of the best choice (i.e. lowest-value) so far for MIN. Alpha-beta search updates the values for $\\alpha$ and $\\beta$ as it goes along and prunes branches once it determines that the value of the state is worse than $\\alpha$ (for MAX) or $\\beta$ (for MIN).\n",
    "\n",
    "Your second task in this assignment is to implement the above behavior. Similar to how minimax is split into 2 functions (depending on the current player), we also split the alpha-beta pruning into 2 functions. For easy reference, modified pseudocode from lecture has been reproduced below. Use this to guide your implementation. Note, we are representing $\\alpha$ as the variable `a` below, and similarly, representing $\\beta$ as the variable `b`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "alphabeta_minimax(game, state, cutoff, depth, eval_fn, a, b)\n",
    "    if cutoff(state, depth) then\n",
    "        return eval_fn(state, player 1), empty_action\n",
    "    else\n",
    "        val = -∞\n",
    "        best_act = empty_action\n",
    "        foreach act in actions(state)\n",
    "            val' = alphabeta_maximin(game, result(state, act), cutoff, depth + 1, eval_fn, a, b)\n",
    "            if val' > val then\n",
    "                val = val'\n",
    "                best_act = act\n",
    "            if val >= b then\n",
    "                return val, best_act\n",
    "            a = max(a, val)\n",
    "    return val, best_act\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "alphabeta_maximin(game, state, cutoff, depth, eval_fn, a, b)\n",
    "    if cutoff(state, depth) then\n",
    "        return eval_fn(state, player 2), empty_action\n",
    "    else\n",
    "        val = ∞\n",
    "        best_act = empty_action\n",
    "        foreach act in actions(state)\n",
    "            val' = alphabeta_minimax(game, result(state, act), cutoff, depth + 1, eval_fn, a, b)\n",
    "            if val' < val then\n",
    "                val = val'\n",
    "                best_act = act\n",
    "            if val <= a then\n",
    "                return val, best_act\n",
    "            b = min(b, val)\n",
    "    return val, best_act\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    ":Minimax search algorithm implentation using alpha-beta pruning\n",
    ":type game: object (an object generated by NeutronGame class)\n",
    ":type state: list (possible sates)\n",
    ":type cutoff: function (optional, customized cutoff function, default by game.terminal_test)\n",
    ":type depth: int (optional, recursion depth, default by 0)\n",
    ":type eval_fn: function (optional, default by game.utility)\n",
    ":type a: float (optional, the value of the best choice so far for MAX, default by negative infinity -> alpha)\n",
    ":type b: float (optional, the value of the best choice so far for MIN, default by positive infinity -> beta)\n",
    ":rtype: tuple (returns a value and an action)\n",
    "\"\"\"\n",
    "def alphabeta_minimax(game, state, cutoff = None, depth = 0, eval_fn = None, a = -math.inf, b = math.inf):\n",
    "    # default cutoff is just a test if the state is terminal\n",
    "    if cutoff == None:\n",
    "        cutoff = lambda s, d : game.terminal_test(s)\n",
    "    \n",
    "    # default eval is just the utility value\n",
    "    if eval_fn == None:\n",
    "        eval_fn = game.utility\n",
    "    \n",
    "    # replace the line below with your code\n",
    "    if cutoff(state, depth):\n",
    "        return eval_fn(state, p1), []\n",
    "    else:\n",
    "        val = float('-inf')\n",
    "        best_act = []\n",
    "        for act in game.actions(state):\n",
    "            val_current = alphabeta_maximin(game, game.result(state, act), cutoff, depth+1, eval_fn, a, b)[0]\n",
    "            if val_current > val:\n",
    "                val = val_current\n",
    "                best_act = act\n",
    "            if val >= b:\n",
    "                return val, best_act\n",
    "            a = max(a, val)\n",
    "    return val, best_act\n",
    "\n",
    "\"\"\"\n",
    ":Maxmin search algorithm implentation using alpha-beta pruning\n",
    ":type game: object (an object generated by NeutronGame class)\n",
    ":type state: list (possible sates)\n",
    ":type cutoff: function (optional, customized cutoff function, default by game.terminal_test)\n",
    ":type depth: int (optional, recursion depth, default by 0)\n",
    ":type eval_fn: function (optional, default by game.utility)\n",
    ":type a: float (optional, the value of the best choice so far for MAX, default by negative infinity -> alpha)\n",
    ":type b: float (optional, the value of the best choice so far for MIN, default by positive infinity -> beta)\n",
    ":rtype: tuple (returns a value and an action)\n",
    "\"\"\"\n",
    "def alphabeta_maximin(game, state, cutoff = None, depth = 0, eval_fn = None, a = -math.inf, b = math.inf):\n",
    "    # default cutoff is just a test if the state is terminal\n",
    "    if cutoff == None:\n",
    "        cutoff = lambda s, d : game.terminal_test(s)\n",
    "    \n",
    "    # default eval is just the utility value\n",
    "    if eval_fn == None:\n",
    "        eval_fn = game.utility\n",
    "    \n",
    "    # replace the line below with your code\n",
    "    if cutoff(state, depth):\n",
    "        return eval_fn(state, p2), []\n",
    "    else:\n",
    "        val = float('inf')\n",
    "        best_act = []\n",
    "        for act in game.actions(state):\n",
    "            val_current = alphabeta_minimax(game, game.result(state, act), cutoff, depth+1, eval_fn, a, b)[0]\n",
    "            if val_current < val:\n",
    "                val = val_current\n",
    "                best_act = act\n",
    "            if val < a:\n",
    "                return val, best_act\n",
    "            b = min(b, val)\n",
    "    return val, best_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a working implementation of __alphabeta_minimax__ and __alphabeta_maximin__, we can create agents based around these algorithms. Just like the minimax agents, these agents can take the optional parameters, but let's not worry about them just yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An agent that utilizes the alpha-beta minimax search algorithm. Can take two optional parameters.\n",
    "cutoff, that can be used to determine when to stop the search\n",
    "eval_fn, which evaluates a state's value from the perspective of a given player\n",
    "\"\"\"\n",
    "class AlphabetaMinimaxAgent:\n",
    "    def __init__(self, cutoff = None, eval_fn = None):\n",
    "        self.cutoff  = cutoff\n",
    "        self.eval_fn = eval_fn\n",
    "\n",
    "    def play_turn(self, game, state):\n",
    "        _, best_act = alphabeta_minimax(game, state, cutoff = self.cutoff, eval_fn = self.eval_fn)\n",
    "        return best_act\n",
    "\n",
    "\"\"\"\n",
    "An agent that utilizes the alpha-beta maximin search algorithm. Can take two optional parameters.\n",
    "cutoff, that can be used to determine when to stop the search\n",
    "eval_fn, which evaluates a state's value from the perspective of a given player\n",
    "\"\"\"\n",
    "class AlphabetaMaximinAgent:\n",
    "    def __init__(self, cutoff = None, eval_fn = None):\n",
    "        self.cutoff  = cutoff\n",
    "        self.eval_fn = eval_fn\n",
    "\n",
    "    def play_turn(self, game, state):\n",
    "        _, best_act = alphabeta_maximin(game, state, cutoff = self.cutoff, eval_fn = self.eval_fn)\n",
    "        return best_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a test, we are going to start from a particular state partway through a game. This particular state was written so that the alpha-beta agent will be able to playout until the end. Run the code cell below to check if your implementation is able to win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------\n",
      "| A |   | B | C | D |\n",
      "---------------------\n",
      "| 1 | * | 2 |   |   |\n",
      "---------------------\n",
      "|   | E |   |   |   |\n",
      "---------------------\n",
      "|   |   |   |   | 3 |\n",
      "---------------------\n",
      "|   | 4 |   | 5 |   |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "---------------------\n",
      "| A | * | B |   | D |\n",
      "---------------------\n",
      "| 1 |   | 2 |   |   |\n",
      "---------------------\n",
      "|   | E |   |   |   |\n",
      "---------------------\n",
      "|   |   |   | C | 3 |\n",
      "---------------------\n",
      "|   | 4 |   | 5 |   |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "\n",
      "Game is over!\n",
      "\n",
      "---------------------\n",
      "| A | * | B |   | D |\n",
      "---------------------\n",
      "| 1 |   | 2 |   |   |\n",
      "---------------------\n",
      "|   | E |   |   |   |\n",
      "---------------------\n",
      "|   |   |   | C | 3 |\n",
      "---------------------\n",
      "|   | 4 |   | 5 |   |\n",
      "---------------------\n",
      "Player to move: Player 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_board = {\n",
    "    (1,1) : PLAYER_1[0], (1,2) : BLANK, (1,3) : PLAYER_1[1], (1,4) : PLAYER_1[2], (1,5) : PLAYER_1[3],\n",
    "    (2,1) : PLAYER_2[0], (2,2) : NEUTRON, (2,3) : PLAYER_2[1], (2,4) : BLANK, (2,5) : BLANK,\n",
    "    (3,1) : BLANK, (3,2) : PLAYER_1[4], (3,3) : BLANK, (3,4) : BLANK, (3,5) : BLANK,\n",
    "    (4,1) : BLANK, (4,2) : BLANK, (4,3) : BLANK, (4,4) : BLANK, (4,5) : PLAYER_2[2],\n",
    "    (5,1) : BLANK, (5,2) : PLAYER_2[3], (5,3) : BLANK, (5,4) : PLAYER_2[4], (5,5) : BLANK,\n",
    "}\n",
    "custom_state = GameState(board = custom_board, player = PLAYER_1, is_initial = False, utility_value = 0)\n",
    "game         = NeutronGame(custom_state)\n",
    "p1           = AlphabetaMinimaxAgent()\n",
    "p2           = RandomAgent()\n",
    "game.play_game(p1, p2, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to have our alpha-beta agent play against a random agent starting from the beginning of the game. When you run the code cell below, you will find that our alpha-beta agent is also unable to process the state space search, much like minimax. Once again, __your Python kernel will crash__. The takeaway from this is that even alpha-beta pruning does not help to make our __Neutron__ domain tractable for adversarial search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-53c40136d6da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mp1\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mAlphabetaMinimaxAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mp2\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mRandomAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-fc5ddaece2d2>\u001b[0m in \u001b[0;36mplay_game\u001b[1;34m(self, first_player, second_player, display_flag, experiment_flag)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mplayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfirst_player\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecond_player\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay_turn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m                 \u001b[0mstate\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdisplay_flag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-b7e15c6b795a>\u001b[0m in \u001b[0;36mplay_turn\u001b[1;34m(self, game, state)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mplay_turn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_act\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malphabeta_minimax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcutoff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbest_act\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-72e0f2b95620>\u001b[0m in \u001b[0;36malphabeta_minimax\u001b[1;34m(game, state, cutoff, depth, eval_fn, a, b)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mbest_act\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mact\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mval_current\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malphabeta_maximin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mval_current\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_current\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-72e0f2b95620>\u001b[0m in \u001b[0;36malphabeta_maximin\u001b[1;34m(game, state, cutoff, depth, eval_fn, a, b)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mbest_act\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mact\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mval_current\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malphabeta_minimax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mval_current\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_current\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "... last 2 frames repeated, from the frame below ...\n",
      "\u001b[1;32m<ipython-input-9-72e0f2b95620>\u001b[0m in \u001b[0;36malphabeta_minimax\u001b[1;34m(game, state, cutoff, depth, eval_fn, a, b)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mbest_act\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mact\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mval_current\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malphabeta_maximin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mval_current\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_current\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "game = NeutronGame()\n",
    "p1   = AlphabetaMinimaxAgent()\n",
    "p2   = RandomAgent()\n",
    "game.play_game(p1, p2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Heuristics\n",
    "\n",
    "<h3 style=\"text-align: right; margin-top: -1em;\">40 points</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there any way to salvage our adversarial search? It turns out, there is! We can cut off our search early and apply a heuristic function to evaluate states, effectively treating non-terminal states as if they were terminal. Of course, this begs the question, what makes a good cutoff test? And what makes a good evaluation function?\n",
    "\n",
    "This is your third task for this assignment. Come up with your own heuristics for the cutoff and evaluation functions so that our agents can finally play the game from start to finish. `custom_cutoff` should return a Boolean value and `custom_eval` should return an integer (positive if the state will lead to a win, negative if the state will lead to a loss).\n",
    "\n",
    "For reference, here is some documentation on a game state:\n",
    "> A state for this game is a 4-tuple, (`board`, `player`, `is_initial`, `utility_value`).\n",
    "> - `board`, a dictionary with (i,j) coordinates as the key, and the piece as\n",
    "  the value. Valid coordinate values for i and j are 1 to 5, inclusive. The piece\n",
    "  should refer to one of the variables `PLAYER_1[n]`, `PLAYER_2[n]`, `NEUTRON`, or\n",
    "  `BLANK`, where `n` can have a value from 0 to 4, inclusive.\n",
    "> - `player`, refers to the player that needs to move in this state. Should reference\n",
    "  one of the variables `PLAYER_1` or `PLAYER_2`.\n",
    "> - `is_initial`, a Boolean value that determines whether or not this state is the\n",
    "  initial state of the game.\n",
    "> - `utility_value`, an integer value that represents the utility value for the\n",
    "  state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    ":To avoid TLE, we customized the following cutoff function:\n",
    ":The game will be terminated if, 1. pass the terminal test (i.e., someone wins) OR, 2. recursion depth has reached to 2\n",
    ":type state: list (possible states)\n",
    ":type depth: int (recursion depth)\n",
    ":rtype: boolean\n",
    "\"\"\"\n",
    "def custom_cutoff(state, depth):\n",
    "    # replace the line below with your code\n",
    "    return game.terminal_test(state) or depth >= 2\n",
    "\n",
    "\"\"\"\n",
    ":My heuristic function is designed to make sure that in the current state,\n",
    ":1. the more number of actions that current player could take, the better\n",
    ":2. the more number of the current player's pieces prevent the neutron from the opponent's home line, the better\n",
    ":3. the less number of the opponent's pieces prevent the neutron from the player's home line, the better\n",
    ":No.2 and No.3 conditions are considered weigh more than No.1, so we multiply them by some parameters (5, 10)\n",
    ":type state: list (possible states)\n",
    ":type player: list (player 1 or player 2)\n",
    ":rtype: float\n",
    "\"\"\"\n",
    "def custom_eval(state, player):\n",
    "    # replace the line below with your code\n",
    "    for loc, blank_piece in state.board.items():\n",
    "        if blank_piece == NEUTRON:\n",
    "            ### neutron coordinates in current state\n",
    "            neutron_x, neutron_y = loc\n",
    "            break\n",
    "            \n",
    "    block_neutron_pieces_num_1 = block_neutron_pieces_num_2 = 0\n",
    "    for x in range(1, neutron_x):\n",
    "        if state.board[(x, neutron_y)] in PLAYER_2:\n",
    "            ### number of player 2's pieces that \"blocks\" player 1 from putting the neutron to the home line\n",
    "            block_neutron_pieces_num_2 += 1\n",
    "    for x in range(neutron_x, 6):\n",
    "        if state.board[(x, neutron_y)] in PLAYER_1:\n",
    "            ### number of player 1's pieces that \"blocks\" player 2 from putting the neutron to the home line\n",
    "            block_neutron_pieces_num_1 += 1\n",
    "    \n",
    "    heuristic = len(game.actions(state)) + 5 * block_neutron_pieces_num_1 - 10 * block_neutron_pieces_num_2\n",
    "    \n",
    "    return heuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate your heuristics! First, we will try the minimax agent against the random agent. (Your kernel should not crash this time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Game is over!\n",
      "\n",
      "---------------------\n",
      "| B |   |   | D | * |\n",
      "---------------------\n",
      "|   | 2 | C |   |   |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "| A | 3 |   |   | E |\n",
      "---------------------\n",
      "| 1 |   |   | 4 | 5 |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "Time elapsed: 0.5625\n"
     ]
    }
   ],
   "source": [
    "game  = NeutronGame()\n",
    "p1    = MinimaxAgent(custom_cutoff, custom_eval)\n",
    "p2    = RandomAgent()\n",
    "start = time.process_time()\n",
    "game.play_game(p1, p2, False)\n",
    "end = time.process_time()\n",
    "print('\\nTime elapsed:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we will try the alpha-beta agent against the random agent. (Again, your kernel should not crash.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Game is over!\n",
      "\n",
      "---------------------\n",
      "| A |   |   |   | E |\n",
      "---------------------\n",
      "|   | B | C |   |   |\n",
      "---------------------\n",
      "| 3 |   |   |   |   |\n",
      "---------------------\n",
      "|   |   |   | D |   |\n",
      "---------------------\n",
      "| 1 | * | 2 | 4 | 5 |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "Time elapsed: 0.15625\n"
     ]
    }
   ],
   "source": [
    "game  = NeutronGame()\n",
    "p1    = AlphabetaMinimaxAgent(custom_cutoff, custom_eval)\n",
    "p2    = RandomAgent()\n",
    "start = time.process_time()\n",
    "game.play_game(p1, p2, False)\n",
    "end = time.process_time()\n",
    "print('\\nTime elapsed:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Experiments & Questions\n",
    "\n",
    "<h3 style=\"text-align: right; margin-top: -1em;\">15 points</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, the game has been using a fixed move ordering of up, down, left, then right. However, it is possible that this move ordering affects the overall performance of our adversarial search. First, let's see how this move ordering affects the number of states generated. To do this, we will play the game between the alpha-beta (with heuristics) agent and the random agent. We will do this repeatedly and then generate a link to a graph of the data. Now, what if we change the move ordering? In the code cell below, provide your own ordering by changing the value of `custom_move_order`, then run the cell. __How did your ordering affect the number of states generated compared to the initial fixed ordering?__ Write your response in the raw text cell below the code cell.\n",
    "\n",
    "_Note_: Running the cell below will take some time.\n",
    "\n",
    "For reference, here is the list of moves as notated in the game logic and what they mean:\n",
    "+ (-1,0) is UP\n",
    "+ (1,0) is DOWN\n",
    "+ (0,-1) is LEFT\n",
    "+ (0,1) is RIGHT\n",
    "\n",
    "Please ensure your choice for `custom_move_order` is different from the initial fixed move order (up, down, left, right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This link is for the number of states generated using the initial move order:\n",
      "https://www.wolframalpha.com/input/?i=plot+%7B4659.1,3585.4,4637.1,5143.2,5583.45,4499.75,5561.5,3560.25,3396.95,5403.75,3865.2,4975.0,4842.6,6335.45,4428.0,3555.4,3840.75,3930.3,4156.5,4884.3%7D\n",
      "\n",
      "This link is for the number of states generated using your custom move order:\n",
      "https://www.wolframalpha.com/input/?i=plot+%7B3848.0,6646.5,3554.05,4703.7,4872.1,4741.5,5145.4,5342.4,4131.25,4432.45,5655.45,4362.3,5518.75,5414.05,5998.05,4702.1,6083.95,4507.2,6027.0,5183.85%7D\n"
     ]
    }
   ],
   "source": [
    "# provide your custom move order below\n",
    "# this should be a list of 4 tuples\n",
    "custom_move_order = [(0,-1), (1,0), (0,1), (-1,0)]\n",
    "\n",
    "# run trials for number of states generated using the initial move order\n",
    "num_trials = 20\n",
    "results    = []\n",
    "for m in range(num_trials):\n",
    "    avg = 0.0\n",
    "    for n in range(num_trials):\n",
    "        game = NeutronGame()\n",
    "        p1   = AlphabetaMinimaxAgent(custom_cutoff, custom_eval)\n",
    "        p2   = RandomAgent()\n",
    "        game.play_game(p1, p2, False, True)\n",
    "        avg += game.num_states_gen\n",
    "    results.append(avg / num_trials)\n",
    "print('This link is for the number of states generated using the initial move order:')\n",
    "s = 'https://www.wolframalpha.com/input/?i=plot+%7B'\n",
    "for r in results:\n",
    "    s += str(r) + ','\n",
    "print(s[:-1] + '%7D')\n",
    "\n",
    "# run trials for number of states generated using the custom move order\n",
    "results = []\n",
    "for m in range(num_trials):\n",
    "    avg = 0.0\n",
    "    for n in range(num_trials):\n",
    "        game = NeutronGame(move_order = custom_move_order)\n",
    "        p1   = AlphabetaMinimaxAgent(custom_cutoff, custom_eval)\n",
    "        p2   = RandomAgent()\n",
    "        game.play_game(p1, p2, False, True)\n",
    "        avg += game.num_states_gen\n",
    "    results.append(avg / num_trials)\n",
    "print('\\nThis link is for the number of states generated using your custom move order:')\n",
    "s = 'https://www.wolframalpha.com/input/?i=plot+%7B'\n",
    "for r in results:\n",
    "    s += str(r) + ','\n",
    "print(s[:-1] + '%7D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**<br/>\n",
    "From the generated plots we can see that after we changed the order somehow, the number of states is **increased** (on average about 4500 using initial order and 5000 after we changed the order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a second experiment, let us take a look at the ordering of our agents. Thus far, we have always been placing our agents as the first player of the game. __What if we place our agent as the second player instead? Does it affect the win rate? How about the number of states generated during the search?__ Run the code cell below to run the experiment and generate links to plot the data. Analyze these plots and provide a short answer in the raw text cell below.\n",
    "\n",
    "_Note_: Running the cell below will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This link is for the win rate as first player:\n",
      "https://www.wolframalpha.com/input/?i=plot+%7B0.7,0.9,0.7,0.8,0.85,0.8,0.8,0.8,0.8,0.8,0.65,0.65,0.85,0.85,0.75,0.8,0.6,0.85,0.95,0.7%7D\n",
      "\n",
      "This link is for the win rate as second player:\n",
      "https://www.wolframalpha.com/input/?i=plot+%7B0.7,0.75,0.75,0.7,0.6,0.65,0.65,0.5,0.55,0.55,0.6,0.45,0.5,0.7,0.65,0.6,0.65,0.4,0.7,0.65%7D\n",
      "\n",
      "This link is for the number of states generated as first player:\n",
      "https://www.wolframalpha.com/input/?i=plot+%7B5023.65,4753.8,5267.55,4469.2,5253.0,4187.0,4926.35,4727.55,3796.85,4286.2,5006.3,4778.05,3575.95,4070.3,5524.75,4294.1,5254.35,4749.3,4103.65,4694.1%7D\n",
      "\n",
      "This link is for the number of states generated as second player:\n",
      "https://www.wolframalpha.com/input/?i=plot+%7B1255.65,1160.1,1315.7,1197.0,1433.45,1069.65,1317.9,1476.05,1395.8,1157.4,1218.4,1300.95,1160.4,1161.9,1059.15,1601.25,1651.9,1105.25,1648.3,1335.4%7D\n"
     ]
    }
   ],
   "source": [
    "# run trials for win rate as first player\n",
    "num_trials = 20\n",
    "results    = []\n",
    "for m in range(num_trials):\n",
    "    win = 0.0\n",
    "    for n in range(num_trials):\n",
    "        game = NeutronGame()\n",
    "        p1   = AlphabetaMinimaxAgent(custom_cutoff, custom_eval)\n",
    "        p2   = RandomAgent()\n",
    "        v    = game.play_game(p1, p2, False, True)\n",
    "        if v == (1,2) or v == (-1,1):\n",
    "            win += 1.0\n",
    "    results.append(win / num_trials)\n",
    "print('This link is for the win rate as first player:')\n",
    "s = 'https://www.wolframalpha.com/input/?i=plot+%7B'\n",
    "for r in results:\n",
    "    s += str(r) + ','\n",
    "print(s[:-1] + '%7D')\n",
    "\n",
    "# run trials for win rate as second player\n",
    "results = []\n",
    "for m in range(num_trials):\n",
    "    win = 0.0\n",
    "    for n in range(num_trials):\n",
    "        game = NeutronGame()\n",
    "        p1   = RandomAgent()\n",
    "        p2   = AlphabetaMaximinAgent(custom_cutoff, custom_eval)\n",
    "        v    = game.play_game(p1, p2, False, True)\n",
    "        if v == (1,1) or v == (-1,2):\n",
    "            win += 1.0\n",
    "    results.append(win / num_trials)\n",
    "print('\\nThis link is for the win rate as second player:')\n",
    "s = 'https://www.wolframalpha.com/input/?i=plot+%7B'\n",
    "for r in results:\n",
    "    s += str(r) + ','\n",
    "print(s[:-1] + '%7D')\n",
    "\n",
    "# run trials for number of states generated as first player\n",
    "results = []\n",
    "for m in range(num_trials):\n",
    "    avg = 0.0\n",
    "    for n in range(num_trials):\n",
    "        game = NeutronGame()\n",
    "        p1   = AlphabetaMinimaxAgent(custom_cutoff, custom_eval)\n",
    "        p2   = RandomAgent()\n",
    "        game.play_game(p1, p2, False, True)\n",
    "        avg += game.num_states_gen\n",
    "    results.append(avg / num_trials)\n",
    "print('\\nThis link is for the number of states generated as first player:')\n",
    "s = 'https://www.wolframalpha.com/input/?i=plot+%7B'\n",
    "for r in results:\n",
    "    s += str(r) + ','\n",
    "print(s[:-1] + '%7D')\n",
    "\n",
    "# run trials for number of states generated as second player\n",
    "results = []\n",
    "for m in range(num_trials):\n",
    "    avg = 0.0\n",
    "    for n in range(num_trials):\n",
    "        game = NeutronGame()\n",
    "        p1   = RandomAgent()\n",
    "        p2   = AlphabetaMaximinAgent(custom_cutoff, custom_eval)\n",
    "        game.play_game(p1, p2, False, True)\n",
    "        avg += game.num_states_gen\n",
    "    results.append(avg / num_trials)\n",
    "print('\\nThis link is for the number of states generated as second player:')\n",
    "s = 'https://www.wolframalpha.com/input/?i=plot+%7B'\n",
    "for r in results:\n",
    "    s += str(r) + ','\n",
    "print(s[:-1] + '%7D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**<br/>\n",
    "From the generated plot we can see that if we change the agent order, **the winning rate decreased from 0.8 to 0.6 on average; the number of states decreased from 4600 to 1300 on average.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one final experiment, let's look at __how the depth limit as the cutoff test can affect our search__. Run the code cell below, analyze the data, and provide your answer in the raw text cell.\n",
    "\n",
    "_Note_: Running the cell below will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This link is for the average win rate as the depth limit increases:\n",
      "https://www.wolframalpha.com/input/?i=plot+%7B0.5,0.9,0.7,0.9,0.9333333333333333%7D\n",
      "\n",
      "This link is for the average number of states generated as the depth limit increases:\n",
      "https://www.wolframalpha.com/input/?i=plot+%7B297.73333333333335,1432.9333333333334,12053.4,53133.333333333336,314442.9666666667%7D\n"
     ]
    }
   ],
   "source": [
    "# run trials for win rate\n",
    "max_depth  = 5\n",
    "results    = []\n",
    "num_trials = 30\n",
    "for depth in range(max_depth):\n",
    "    exp_cutoff = lambda s, d : d >= depth + 1\n",
    "    win        = 0.0\n",
    "    for n in range(num_trials):\n",
    "        game = NeutronGame()\n",
    "        p1   = AlphabetaMinimaxAgent(exp_cutoff, custom_eval)\n",
    "        p2   = RandomAgent()\n",
    "        v    = game.play_game(p1, p2, False, True)\n",
    "        if v == (1,2) or v == (-1,1):\n",
    "            win += 1.0\n",
    "    results.append(win / num_trials)\n",
    "print('This link is for the average win rate as the depth limit increases:')\n",
    "s = 'https://www.wolframalpha.com/input/?i=plot+%7B'\n",
    "for r in results:\n",
    "    s += str(r) + ','\n",
    "print(s[:-1] + '%7D')\n",
    "\n",
    "# run trials for number of states generated\n",
    "results = []\n",
    "for depth in range(max_depth):\n",
    "    exp_cutoff = lambda s, d : d >= depth + 1\n",
    "    avg        = 0.0\n",
    "    for n in range(num_trials):\n",
    "        game = NeutronGame()\n",
    "        p1   = AlphabetaMinimaxAgent(exp_cutoff, custom_eval)\n",
    "        p2   = RandomAgent()\n",
    "        game.play_game(p1, p2, False, True)\n",
    "        avg += game.num_states_gen\n",
    "    results.append(avg / num_trials)\n",
    "print('\\nThis link is for the average number of states generated as the depth limit increases:')\n",
    "s = 'https://www.wolframalpha.com/input/?i=plot+%7B'\n",
    "for r in results:\n",
    "    s += str(r) + ','\n",
    "print(s[:-1] + '%7D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**<br/>\n",
    "From the plots we can find that generally, **increasing depth limit can definitely result in an increasing of winning rate. However, the winning rate converges when the depth limit reaches to some point** (2-3 in this case, that's why I set the customized cutoff depth to be 2).<br/>\n",
    "**For average number of states, it keeps increasing as depth goes up.** Also, it seems that the speed is growing exponentially somehow (won't converge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Just for Fun\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to close this assignment, you are now tasked with creating 2 additional game states. For the first game state, our pure minimax agent (meaning no heuristics) should be able to playout as the first player. For the second game state, the pure alpha-beta agent should be able to playout as the first player, but the pure minimax agent should not be able to. Good luck! :)\n",
    "\n",
    "__Do not use the game state from earlier in the assignment!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Game is over!\n",
      "\n",
      "---------------------\n",
      "|   | 2 | D | * |   |\n",
      "---------------------\n",
      "| B |   | 3 |   | C |\n",
      "---------------------\n",
      "| E |   |   |   |   |\n",
      "---------------------\n",
      "| 1 |   |   |   | 4 |\n",
      "---------------------\n",
      "|   |   |   | A | 5 |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "Congrats! The minimax agent was able to win!\n",
      "\n",
      "Game is over!\n",
      "\n",
      "---------------------\n",
      "|   | B | D |   | * |\n",
      "---------------------\n",
      "|   |   |   | 4 |   |\n",
      "---------------------\n",
      "|   |   | C |   | E |\n",
      "---------------------\n",
      "|   |   | A |   |   |\n",
      "---------------------\n",
      "| 1 | 2 |   | 3 | 5 |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "Congrats! The alpha-beta agent was able to win!\n",
      "But you make sure that your minimax agent cannot [as in, it should kill the Python kernel].\n"
     ]
    }
   ],
   "source": [
    "minimax_board = {\n",
    "    (1,1) : PLAYER_1[0], (1,2) : PLAYER_1[1], (1,3) : PLAYER_1[2], (1,4) : PLAYER_1[3], (1,5) : PLAYER_1[4],\n",
    "    (2,1) : BLANK, (2,2) : BLANK, (2,3) : BLANK, (2,4) : BLANK, (2,5) : BLANK,\n",
    "    (3,1) : BLANK, (3,2) : BLANK, (3,3) : NEUTRON, (3,4) : BLANK, (3,5) : BLANK,\n",
    "    (4,1) : BLANK, (4,2) : BLANK, (4,3) : BLANK, (4,4) : BLANK, (4,5) : BLANK,\n",
    "    (5,1) : PLAYER_2[0], (5,2) : PLAYER_2[1], (5,3) : PLAYER_2[2], (5,4) : PLAYER_2[3], (5,5) : PLAYER_2[4]\n",
    "}\n",
    "minimax_state = GameState(board = minimax_board, player = PLAYER_1, is_initial = False, utility_value = 0)\n",
    "game          = NeutronGame(minimax_state)\n",
    "p1            = MinimaxAgent(custom_cutoff, custom_eval)\n",
    "p2            = RandomAgent()\n",
    "v             = game.play_game(p1, p2, False)\n",
    "if v == (1,2) or v == (-1,1):\n",
    "    print('Congrats! The minimax agent was able to win!')\n",
    "\n",
    "alphabeta_board = {\n",
    "    (1,1) : PLAYER_1[0], (1,2) : PLAYER_1[1], (1,3) : PLAYER_1[2], (1,4) : PLAYER_1[3], (1,5) : PLAYER_1[4],\n",
    "    (2,1) : BLANK, (2,2) : BLANK, (2,3) : BLANK, (2,4) : BLANK, (2,5) : BLANK,\n",
    "    (3,1) : BLANK, (3,2) : BLANK, (3,3) : NEUTRON, (3,4) : BLANK, (3,5) : BLANK,\n",
    "    (4,1) : BLANK, (4,2) : BLANK, (4,3) : BLANK, (4,4) : BLANK, (4,5) : BLANK,\n",
    "    (5,1) : PLAYER_2[0], (5,2) : PLAYER_2[1], (5,3) : PLAYER_2[2], (5,4) : PLAYER_2[3], (5,5) : PLAYER_2[4]\n",
    "}\n",
    "alphabeta_state = GameState(board = alphabeta_board, player = PLAYER_1, is_initial = False, utility_value = 0)\n",
    "game            = NeutronGame(alphabeta_state)\n",
    "p1              = AlphabetaMinimaxAgent(custom_cutoff, custom_eval)\n",
    "p2              = RandomAgent()\n",
    "v               = game.play_game(p1, p2, False)\n",
    "if v == (1,2) or v == (-1,1):\n",
    "    print('Congrats! The alpha-beta agent was able to win!')\n",
    "    print('But you make sure that your minimax agent cannot [as in, it should kill the Python kernel].')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will only be submitting your Jupyter notebook file, *hw3.ipynb*. Do not worry about submitting the additional *game_tree.png* file. Furthermore, as a reminder, part of your grade is your documentation. Each of the functions you implemented as part of this assignment **must** be documented.\n",
    "\n",
    "Please upload your *hw3.ipynb* file to CMS by **Monday, March 4 @ 1:24pm**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
