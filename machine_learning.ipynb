{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 4700 - Homework 5: Supervised Learning\n",
    "<h3 style=\"color: red;\">\n",
    "    Due: Wednesday, April 17, 1:24pm on CMS\n",
    "    <br><br>\n",
    "    Late submission: Friday, April 19, 1:24pm on CMS (50% penalty)\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which other students did you interact with on this assignment? Provide their NetID(s) below consistent with the [homework policy](http://www.cs.cornell.edu/courses/cs4700/2019sp/)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "xl738, jd952"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 5 will provide you with programming practice in the supervised learning environment. You will be implementing algorithms in Python 3, so please ensure your Jupyter Notebook environment is running the Python 3 kernel. Start by running the cell below. These are the **only** `import` statements allowed in this assignment. **Importing additional libraries will result in an automatic 0!**\n",
    "\n",
    "Throughout the assignment, you will find sections labeled as **<span style=\"color: red;\">Task</span>**. These sections mark the parts that you will need to work on. The other, un-marked sections simply provide information on the topics for reference.\n",
    "\n",
    "Lastly, most of your implementations for this homework will need to use functions from the `math` module. For those unfamiliar with the functions offered by this module, please refer to its documentation here: [https://docs.python.org/3/library/math.html](https://docs.python.org/3/library/math.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math    # copysign, exp, inf, log, pow\n",
    "import random  # shuffle, uniform\n",
    "import sys     # version_info\n",
    "\n",
    "assert sys.version_info >= (3,), 'You must use Python 3 for this assignment!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Logistic Regression\n",
    "\n",
    "<h3 style=\"text-align: right; margin-top: -1em;\">20 points</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the **perceptron linear classifier** from lecture. The perceptron is a **single-layer neural network** that uses a hard threshold, which can cause issues for more complex data sets. One specific downside of this classifier is that it always outputs a confident result of 0 or 1; however, there are many cases where we would instead like to return a granular output, such as a probability. We can fix these issues by replacing the hard threshold function with a soft threshold function. One such function is the **logistic function**: $g \\left( z \\right) = \\frac{1}{1 + e^{-z}}$. The figure below shows a comparison between the two types of threshold functions.\n",
    "\n",
    "![](hard_vs_soft_threshold.png)\n",
    "\n",
    "Utilizing the logistic function as our threshold for a single-layer neural network results in the **logistic regression classifier**:\n",
    "\n",
    "$$\n",
    "h_\\overline{w} \\left( \\overline{x} \\right) = g \\left( \\overline{w} \\cdot \\overline{x} \\right) = \\frac{1}{1 + e^{-\\overline{w} \\cdot \\overline{x}}}\n",
    "$$\n",
    "\n",
    "In order for this model to learn the appropriate set of weights, we use the gradient descent method to minimize the error that our model makes. Given that the derivative of the logistic function is $g^\\prime \\left( z \\right) = g \\left( z \\right) \\times \\left( 1 - g \\left(z \\right) \\right)$, we can compute the gradient update for each weight (and for some learning rate $\\alpha$) as:\n",
    "\n",
    "$$\n",
    "w_i \\leftarrow w_i + \\alpha \\times \\left( y - g \\left( \\overline{w} \\cdot \\overline{x} \\right) \\right) \\times g^\\prime \\left(\\overline{w} \\cdot \\overline{x} \\right) \\times x_i\n",
    "$$\n",
    "\n",
    "Or equivalenty, in terms of only the logistic function, $g \\left( z \\right)$:\n",
    "\n",
    "$$\n",
    "w_i \\leftarrow w_i + \\alpha \\times \\left( y - g \\left( \\overline{w} \\cdot \\overline{x} \\right) \\right) \\times g \\left(\\overline{w} \\cdot \\overline{x} \\right) \\times \\left( 1 - g \\left( \\overline{w} \\cdot \\overline{x} \\right) \\right) \\times x_i\n",
    "$$\n",
    "\n",
    "Recall that the weights have an explicit \"bias\" term as $w_0$. This term has a corresponding entry in each example as a dummy value of 1 (i.e., $x_{i0} = 1$ for all entries $i$ in the data set). The figure below shows an abstract representation of our classifier.\n",
    "\n",
    "![](logistic_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: red;\">Task:</h2>\n",
    "\n",
    "Implement the logistic regression classifier, using the pseudocode below as your guide. To help, you will need to implement two additional helper functions. `logistic` should implement the logistic function (refer to the previous formula). `dot_prod` should calculate the dot product of the two input vectors (leave the `assert` statement).\n",
    "\n",
    "---\n",
    "\n",
    "**Note, the pseudocode below is *slightly* different from what you saw in lecture. Here, we specify the stopping condition for the classifier as just iterating over the data set some number of times. This information is passed in as input to the classifier as `epochs`.**\n",
    "\n",
    "---\n",
    "\n",
    "$\n",
    "\\text{Inputs:} \\\\\n",
    "D - \\text{data set, a list of } \\left( \\overline{x},y \\right) \\text{, where } \\overline{x} \\in \\mathbb{R}^n \\text{ and } y \\in \\{0,1\\} \\\\\n",
    "\\alpha - \\text{learning rate} \\\\\n",
    "\\text{epochs} - \\text{total number of iterations} \\\\\n",
    "\\ \\\\\n",
    "\\overline{\\underline{\\textbf{logistic_learn} \\left( D, \\alpha, \\text{epochs} \\right)}} \\\\\n",
    "\\quad \\overline{w} \\leftarrow \\overline{0} \\text{ of size } n \\\\\n",
    "\\quad \\textbf{while} \\text{ epochs} > 0 \\\\\n",
    "\\qquad \\textbf{for each} \\left( \\overline{x},y \\right) \\in D \\\\\n",
    "\\qquad \\quad p \\leftarrow g \\left( \\overline{w} \\cdot \\overline{x} \\right) \\\\\n",
    "\\qquad \\quad \\textbf{for each} \\text{ feature } i \\\\\n",
    "\\qquad \\qquad w_i \\leftarrow w_i + \\alpha \\times \\left( y - p \\right) \\times p \\times \\left( 1 - p \\right) \\times x_i \\\\\n",
    "\\qquad \\text{epochs} \\leftarrow \\text{epochs} - 1 \\\\\n",
    "\\quad \\textbf{return } \\overline{w}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Logistic (sigmoid) function\n",
    "\n",
    "Inputs:\n",
    "  z - logistic input\n",
    "\"\"\"\n",
    "def logistic(z):\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "\n",
    "\"\"\"\n",
    "Dot product function\n",
    "Inputs should have the same size\n",
    "\n",
    "Inputs:\n",
    "  xs - the first vector\n",
    "  ys - the second vector\n",
    "\"\"\"\n",
    "def dot_prod(xs, ys):\n",
    "    assert len(xs) == len(ys), 'Vector inputs must be of same size!'\n",
    "    res = 0\n",
    "    for i in range(len(xs)):\n",
    "        res += xs[i] * ys[i]\n",
    "    return res\n",
    "\n",
    "\"\"\"\n",
    "Logistic regression classifier\n",
    "\n",
    "Inputs:\n",
    "  data_set - data set, a list of data points (x, y), where x is a vector of size n, y is either 0 or 1\n",
    "  learning_rate - learning rate\n",
    "  epochs - total number of iterations (of whole data set)\n",
    "\"\"\"\n",
    "def logistic_learn(data_set, learning_rate, epochs):\n",
    "    w = [0] * len(data_set[0][0])\n",
    "    while epochs > 0:\n",
    "        for x, y in data_set:\n",
    "            p = logistic(dot_prod(w, x))\n",
    "            for i in range(len(data_set[0][0])):\n",
    "                w[i] += learning_rate * (y-p) * p * (1-p) * x[i]\n",
    "        epochs -= 1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sanity check, we have included some test cases that you can run your implementation against. The cell below will pass in a data set containing the 4 cases for the logical **AND** function. The logistic regression classifier should be able to learn the appropriate weights so that the 4 predictions are close to the actual labels. Try changing the learning rate and the number of epochs to get more accurate predictions. Your predictions should be within 0.1 of the true label for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AND 0 is 0, you predicted 1.6490873234046917e-05\n",
      "0 AND 1 is 0, you predicted 0.02345358858025149\n",
      "1 AND 0 is 0, you predicted 0.02346127457286197\n",
      "1 AND 1 is 1, you predicted 0.9722134956337248\n"
     ]
    }
   ],
   "source": [
    "and_set = [([1, 0, 0], 0), ([1, 0, 1], 0), ([1, 1, 0], 0), ([1, 1, 1], 1)]\n",
    "weights = logistic_learn(and_set, 0.5, 10000)\n",
    "for (x, y) in and_set:\n",
    "    pred = logistic(dot_prod(weights, x))\n",
    "    print('{} AND {} is {}, you predicted {}'.format(x[1], x[2], y, pred))\n",
    "    assert abs(pred - y) < 0.1, 'Your prediction diverged too much from the true label!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's ensure that your classifier is also able to learn the weights for the logical **OR** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 OR 0 is 0, you predicted 0.02331033960673716\n",
      "0 OR 1 is 1, you predicted 0.9853372900246228\n",
      "1 OR 0 is 1, you predicted 0.9853342752458599\n",
      "1 OR 1 is 1, you predicted 0.9999947138561152\n"
     ]
    }
   ],
   "source": [
    "or_set = [([1, 0, 0], 0), ([1, 0, 1], 1), ([1, 1, 0], 1), ([1, 1, 1], 1)]\n",
    "weights = logistic_learn(or_set, 0.5, 10000)\n",
    "for (x, y) in or_set:\n",
    "    pred = logistic(dot_prod(weights, x))\n",
    "    print('{} OR {} is {}, you predicted {}'.format(x[1], x[2], y, pred))\n",
    "    assert abs(pred - y) < 0.1, 'Your prediction diverged too much from the true label!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression classifier is still constrained in what functions it can represent. For example, it still fails to find weights for non-linearly separable data sets. The common example of such a data set is **XOR**. Run the cell below to verify for yourself that our logistic regression classifier is unable to produce correct weights that approximate the logical XOR function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 XOR 0 is 0, you predicted 0.51610600358629\n",
      "0 XOR 1 is 1, you predicted 0.5\n",
      "1 XOR 0 is 1, you predicted 0.4838939964137099\n",
      "1 XOR 1 is 0, you predicted 0.46782138179306076\n"
     ]
    }
   ],
   "source": [
    "xor_set = [([1, 0, 0], 0), ([1, 0, 1], 1), ([1, 1, 0], 1), ([1, 1, 1], 0)]\n",
    "weights = logistic_learn(xor_set, 0.5, 10000)\n",
    "for (x, y) in xor_set:\n",
    "    pred = logistic(dot_prod(weights, x))\n",
    "    print('{} XOR {} is {}, you predicted {}'.format(x[1], x[2], y, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Deep Neural Networks\n",
    "\n",
    "<h3 style=\"text-align: right; margin-top: -1em;\">40 points</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To alleviate our issue of non-linearity, we need to replace our shallow, single-layer model. Both the perceptron and logistic regression classifiers have a single input layer and a single output node. But, to capture the non-linear relationship in data, we need additional layers! We refer to these, in general, as **hidden layers**. Combining this deep model of multiple layers with a soft threshold function (**activation function**) results in a **deep (multi-layered) neural network** (**DNN**). For simplicity, we will that each layer is fully connected to all nodes in the layer above it, allowing them to be represented as directed acyclic graphs of **units** or **neurons**. Another characteristic of DNNs is that they allow us to have multiple outputs, which is different from the classifiers we've explored so far. As a consequence, we now treat a data set $D$ to consist of $\\left(\\overline{x},\\overline{y}\\right)$, meaning both the data and label are vectors.\n",
    "\n",
    "The figure below shows an example of a DNN with 1 input layer (2 input units), 1 hidden layer (2 hidden units), and 1 output unit.\n",
    "\n",
    "![](deep_neural_network.png)\n",
    "\n",
    "When an input is passed into a DNN, the signals are passed along the network by following the connections between the units, starting at the input layer and ending in the output layer. This process is refered to as **forward propagation**. In order for the DNN to learn, it must update its weights across its multiple layers. The common approach for DNN learning is called **back-propagation**, which relies on the gradient descent method.\n",
    "\n",
    "The code cell below defines the `DNNUnit` class, which will represent a unit in a DNN. Creating an actual DNN is handled by the function `create_DNN`. Read through and understand the code below. **Of important note is the way the bias unit is handled: every unit in the network (except those in the input layer) has the bias unit as the first input/weight.** Run the cell to include these definitions to the environment.\n",
    "\n",
    "As an example, we can define the network from the figure above (assuming the logistic function is the activation function) as the following:\n",
    "```python\n",
    "logistic_deriv = lambda z : logistic(z) * (1 - logistic(z))\n",
    "figure_network = create_DNN(2, [2], 1, logistic, logistic_deriv)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A unit of a neural network.\n",
    "\n",
    "Inputs:\n",
    "  activation - the activation function to be used\n",
    "  deriv      - the derivative of the activation function\n",
    "  inputs     - a list of units that feed a signal to this unit\n",
    "  weights    - a list of weights that correspond to each input signal\n",
    "\"\"\"\n",
    "class DNNUnit:\n",
    "    def __init__(self, activation, deriv, inputs = None, weights = None):\n",
    "        self.activation = activation\n",
    "        self.deriv = deriv\n",
    "        self.inputs = inputs or []\n",
    "        self.weights = weights or []\n",
    "        self.value = None\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Creates a deep, multi-layered neural network. Includes a bias unit of value 1\n",
    "that is connected to every unit except those in the input layer. Initializes\n",
    "all weights to a random real number between -0.5 and 0.5, inclusive.\n",
    "\n",
    "Inputs:\n",
    "    input_layer_size   - the number of input units for the network\n",
    "    hidden_layer_sizes - a list with the number of hidden units in each hidden layer\n",
    "    output_layer_size  - the number of output units for the network\n",
    "    activation         - the activation function to be used\n",
    "    deriv              - the derivative of the activation function\n",
    "\"\"\"\n",
    "def create_DNN(input_layer_size, hidden_layer_sizes, output_layer_size, activation, deriv):\n",
    "    layer_sizes = [input_layer_size] + hidden_layer_sizes + [output_layer_size]\n",
    "    network = [[DNNUnit(activation, deriv) for _ in range(s)] for s in layer_sizes]\n",
    "    dummy_unit = DNNUnit(activation, deriv)\n",
    "    dummy_unit.value = 1.0\n",
    "    \n",
    "    # create weighted links\n",
    "    for k in range(1, len(layer_sizes)):\n",
    "        for unit in network[k]:\n",
    "            unit.inputs.append(dummy_unit)\n",
    "            unit.weights.append(random.uniform(-0.5, 0.5))\n",
    "            for input_unit in network[k - 1]:\n",
    "                unit.inputs.append(input_unit)\n",
    "                unit.weights.append(random.uniform(-0.5, 0.5))\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worthwhile to explore the interconnectedness between our above implementation of a DNN and its mathematical representation. The table below shows the attributes of a unit $i$ in a DNN, how to access it in code (let the variable be called `unit`), and its mathematical representation:\n",
    "\n",
    "| Description | Code | Math |\n",
    "| ----------- | ---- | ---- |\n",
    "| Activation function | `unit.activation` | $g\\left(z\\right)$ |\n",
    "| Derivative of activation function | `unit.deriv` | $g^\\prime\\left(z\\right)$ |\n",
    "| Units that connect to $i$ | `unit.inputs` | $\\overline{s}_i$ |\n",
    "| Weights for those units in $\\overline{s}_i$ | `unit.weights` | $\\overline{v}_i$ |\n",
    "| Value of the unit $i$ | `unit.value` | $a_i$ |\n",
    "\n",
    "Typically, when discussing about a unit $i$ in layer $k$, we also need to talk about the units that connect to $i$, the units that $i$ connects to, and their associated attributes. The table below summarizes the specific relationships we need for the propagation algorithms:\n",
    "\n",
    "| Description | Math |\n",
    "| ----------- | ---- |\n",
    "| Values for those units in $\\overline{s}_i$ | $\\overline{p}_i$ |\n",
    "| Units in layer $k+1$ that $i$ connects to | $\\overline{t}_i$ |\n",
    "| Weights between $i$ and those units in $\\overline{t}_i$ | $\\overline{u}_i$ |\n",
    "| Error for $i$ | $\\Delta_i$ |\n",
    "| Errors of those units in $\\overline{t}_i$ | $\\overline{q}_i$ |\n",
    "\n",
    "To make the above information slightly more concrete, let's look at a brief example. Consider the figure below. Note that unit 0 refers to the bias unit.\n",
    "\n",
    "![](dnn_example.png)\n",
    "\n",
    "For the figure above, we calculate the relevant information for unit 3 in the table below.\n",
    "\n",
    "| Math | Value |\n",
    "| ---- | ----- |\n",
    "| $\\overline{s}_3$ | $\\left<\\text{Unit 0, Unit 1, Unit 2}\\right>$ |\n",
    "| $\\overline{v}_3$ | $\\left<w_{0,3}\\text{, }w_{1,3}\\text{, }w_{2,3}\\right>$ |\n",
    "| $\\overline{p}_3$ | $\\left<a_0\\text{, }a_1\\text{, }a_2\\right>$ |\n",
    "| $\\overline{t}_3$ | $\\left<\\text{Unit 4, Unit 5, Unit 6}\\right>$ |\n",
    "| $\\overline{u}_3$ | $\\left<w_{3,4}\\text{, }w_{3,5}\\text{, }w_{3,6}\\right>$ |\n",
    "| $\\overline{q}_3$ | $\\left<\\Delta_4\\text{, }\\Delta_5\\text{, }\\Delta_6\\right>$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: red;\">Task:</h2>\n",
    "\n",
    "Implement the propagation algorithms, `forward_prop` and `back_prop`, using the pseudocode below as a guide.\n",
    "\n",
    "$\n",
    "\\text{Inputs:} \\\\\n",
    "\\text{DNN} - \\text{current state of the deep neural network,} \\\\\n",
    "\\quad \\text{where each layer } k \\text{ consists of units and each unit } i \\text{ has:} \\\\\n",
    "\\qquad \\bullet \\text{activation function } g \\\\\n",
    "\\qquad \\bullet \\text{derivative of activation function } g^\\prime \\\\\n",
    "\\qquad \\bullet \\text{vector of inputs } \\overline{s}_i \\\\\n",
    "\\qquad \\bullet \\text{vector of weights } \\overline{v}_i \\\\\n",
    "\\qquad \\bullet \\text{value } a_i \\\\\n",
    "\\overline{x} - \\text{current vector sample to be forward propagated} \\\\\n",
    "\\ \\\\\n",
    "\\overline{\\underline{\\textbf{forward_prop} \\left( \\text{DNN, } \\overline{x} \\right)}} \\\\\n",
    "\\quad \\textbf{for each} \\text{ unit } i \\in \\text{input layer} \\\\\n",
    "\\qquad a_i \\leftarrow x_i \\\\\n",
    "\\quad \\textbf{for each} \\text{ layer } k \\in \\text{DNN starting at second layer to output layer} \\\\\n",
    "\\qquad \\textbf{for each} \\text{ unit } i \\in k \\\\\n",
    "\\qquad \\quad \\overline{p}_i \\leftarrow \\text{vector of values for units in } \\overline{s}_i \\\\\n",
    "\\qquad \\quad a_i \\leftarrow g \\left( \\overline{v}_i \\cdot \\overline{p}_i \\right) \\\\\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "$\n",
    "\\text{Inputs:} \\\\\n",
    "\\text{DNN} - \\text{current state of the deep neural network,} \\\\\n",
    "\\quad \\text{where each layer } k \\text{ consists of units and each unit } i \\text{ has:} \\\\\n",
    "\\qquad \\bullet \\text{activation function } g \\\\\n",
    "\\qquad \\bullet \\text{derivative of activation function } g^\\prime \\\\\n",
    "\\qquad \\bullet \\text{vector of inputs } \\overline{s}_i \\\\\n",
    "\\qquad \\bullet \\text{vector of weights } \\overline{v}_i \\\\\n",
    "\\qquad \\bullet \\text{value } a_i \\\\\n",
    "D - \\text{data set, a list of} \\left( \\overline{x} , \\overline{y} \\right), \\text{where } \\overline{x} \\in \\mathbb{R}^{n} \\text{ and } \\overline{y} \\in \\mathbb{R}^{d} \\\\\n",
    "\\quad \\left( \\text{meaning } n \\text{ features and } d \\text{ number of outputs} \\right) \\\\\n",
    "\\alpha - \\text{learning rate} \\\\\n",
    "\\text{epochs} - \\text{total number of iterations} \\\\\n",
    "\\ \\\\\n",
    "\\overline{\\underline{\\textbf{back_prop} \\left( \\text{DNN,} D, \\alpha, \\text{epochs} \\right)}} \\\\\n",
    "\\quad \\Delta \\leftarrow \\text{vector of errors, initialized with units in DNN as keys and 0 for values} \\\\\n",
    "\\quad \\textbf{while } \\text{epochs} > 0 \\\\\n",
    "\\qquad \\textbf{for each} \\left( \\overline{x} , \\overline{y} \\right) \\in D \\\\\n",
    "\\qquad \\quad \\textbf{forward_prop} \\left( \\text{DNN, } \\overline{x} \\right) \\\\\n",
    "\\qquad \\quad \\textbf{for each} \\text{ unit } i \\in \\text{output layer} \\\\\n",
    "\\qquad \\qquad \\overline{p}_i \\leftarrow \\text{vector of values for units in } \\overline{s}_i \\\\\n",
    "\\qquad \\qquad \\Delta_i \\leftarrow g^\\prime \\left( \\overline{v}_i \\cdot \\overline{p}_i \\right) \\times \\left( y_i - a_i \\right) \\\\\n",
    "\\qquad \\quad \\textbf{for each} \\text{ layer } k \\text{ starting from last hidden layer to one above input layer} \\\\\n",
    "\\qquad \\qquad \\textbf{for each} \\text{ unit } i \\in k \\\\\n",
    "\\qquad \\qquad \\quad \\overline{p}_i \\leftarrow \\text{vector of values for units in } \\overline{s}_i \\\\\n",
    "\\qquad \\qquad \\quad \\overline{t}_i \\leftarrow \\text{vector of units in layer } k+1 \\text{ that } i \\text{ connects to} \\\\\n",
    "\\qquad \\qquad \\quad \\overline{u}_i \\leftarrow \\text{vector of weights between } i \\text{ and those units in } \\overline{t}_i \\\\\n",
    "\\qquad \\qquad \\quad \\overline{q}_i \\leftarrow \\text{vector of errors of those units in } \\overline{t}_i \\\\\n",
    "\\qquad \\qquad \\quad \\Delta_i \\leftarrow g^\\prime \\left( \\overline{v}_i \\cdot \\overline{p}_i \\right) \\times \\left( \\overline{u}_i \\cdot \\overline{q}_i \\right) \\\\\n",
    "\\qquad \\quad \\textbf{for each} \\text{ weight } w_{ij} \\text{ in DNN} \\\\\n",
    "\\qquad \\qquad w_{ij} \\leftarrow w_{ij} + \\alpha \\times a_i \\times \\Delta_j \\\\\n",
    "\\qquad \\text{epochs} \\leftarrow \\text{epochs} - 1\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Forward propagation algorithm\n",
    "\n",
    "Inputs:\n",
    "  network - DNN, current state of the neural network\n",
    "  x - current vector sample (data point) to be forward propagated\n",
    "\"\"\"\n",
    "def forward_prop(network, x):\n",
    "    for i in range(len(network[0])):\n",
    "        network[0][i].value = x[i]\n",
    "    for k in range(1, len(network)):\n",
    "        for i in range(len(network[k])):\n",
    "            p = [j.value for j in network[k][i].inputs]\n",
    "            network[k][i].value = network[k][i].activation(dot_prod(network[k][i].weights, p))\n",
    "\"\"\"\n",
    "Back propagation algorithm\n",
    "\n",
    "Inputs: \n",
    "  network - DNN, current state of the neural network\n",
    "  data_set - data set, a list of data points (x, y), where x is a vector of size n, y is a vector of size d\n",
    "             (meaning n features and d number of outputs)\n",
    "  learning_rate - learning rate\n",
    "  epochs - total number of iterations (of whole data set)\n",
    "\"\"\"\n",
    "def back_prop(network, data_set, learning_rate, epochs):\n",
    "    delta = {}\n",
    "    for layer in network:\n",
    "        for unit in layer:\n",
    "            delta[unit] = 0\n",
    "    while epochs > 0:\n",
    "        for x, y in data_set:\n",
    "            forward_prop(network, x)\n",
    "            for i in range(len(network[-1])):\n",
    "                p = [j.value for j in network[-1][i].inputs]\n",
    "                delta[network[-1][i]] = network[-1][i].deriv(dot_prod(network[-1][i].weights, p)) * (y[i] - network[-1][i].value)\n",
    "            for k in range(len(network)-2, 0, -1):\n",
    "                for i in range(len(network[k])):\n",
    "                    p = [m.value for m in network[k][i].inputs]\n",
    "                    u, q = [], []\n",
    "                    for j in range(len(network[k+1])): \n",
    "                        u.append(network[k+1][j].weights[i+1])\n",
    "                        q.append(delta[network[k+1][j]])\n",
    "                    delta[network[k][i]] = network[k][i].deriv(dot_prod(network[k][i].weights, p)) * dot_prod(u, q)\n",
    "            for k in range(len(network)-1):\n",
    "                for j in range(len(network[k+1])):\n",
    "                    network[k+1][j].weights[0] += learning_rate * network[k+1][j].inputs[0].value * delta[network[k+1][j]]\n",
    "                for i in range(len(network[k])):\n",
    "                    for j in range(len(network[k+1])):\n",
    "                        network[k+1][j].weights[i+1] += learning_rate * network[k][i].value * delta[network[k+1][j]]\n",
    "        epochs -= 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: red;\">Task:</h2>\n",
    "\n",
    "If you have a working implementation of the algorithms, the DNN should be able to learn the **2-bit adder** function. For those unfamiliar, the following table summarizes this function's behavior:\n",
    "\n",
    "| $x_1$ | $x_2$ | $y_1$ | $y_2$ |\n",
    "| --- | --- | --- | --- |\n",
    "| 0 | 0 | 0 | 0 |\n",
    "| 0 | 1 | 0 | 1 |\n",
    "| 1 | 0 | 0 | 1 |\n",
    "| 1 | 1 | 1 | 0 |\n",
    "\n",
    "In short, the 2-bit adder has 2 outputs, where $y_1$ represents logical **AND** (linearly separable) and $y_2$ represents logical **XOR** (non-linearly separable). Run the code cell below multiple times to ensure your implementation is able to learn the 2-bit adder function. It is possible that the DNN will not approximate the function within an acceptable error-bound, even though your implementation is correct. **Why does this possibility exist?** Write your reasoning in the raw cell below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As long as the number of traing data points is not large enough (corresponding to the error bound), there is possiblity that the neural network cannot approximate the funciton (AND/XOR) accurately enough. Since in this case we only have 4 data points in our training set, it is likely that our test data in the very \"edge case\". In other words, the model can be under-fitting. Hence it is possible that DNN will not approximate the function within an acceptable error-bound. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input = [0, 0]\n",
      "True Output = [0, 0]\n",
      "Prediction = [0.004673143865782167, 0.01640349469266924]\n",
      "\n",
      "Input = [0, 1]\n",
      "True Output = [0, 1]\n",
      "Prediction = [0.010015474955235049, 0.9838248613732609]\n",
      "\n",
      "Input = [1, 0]\n",
      "True Output = [0, 1]\n",
      "Prediction = [0.01000789095795973, 0.9836640659090724]\n",
      "\n",
      "Input = [1, 1]\n",
      "True Output = [1, 0]\n",
      "Prediction = [0.981973362611428, 0.019509757094707494]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_set = [([0,0],[0,0]), ([0,1],[0,1]), ([1,0],[0,1]), ([1,1],[1,0])]\n",
    "test_set = [[0,0], [0,1], [1,0], [1,1]]\n",
    "correct_out = [[0,0], [0,1], [0,1], [1,0]]\n",
    "logistic_deriv = lambda z : logistic(z) * (1 - logistic(z))\n",
    "network = create_DNN(2, [2,2], 2, logistic, logistic_deriv)                        \n",
    "back_prop(network, data_set, 0.5, 10000)                        \n",
    "for t in range(len(test_set)):\n",
    "    print('Input =', test_set[t])\n",
    "    print('True Output =', correct_out[t])\n",
    "    forward_prop(network, test_set[t])\n",
    "    preds = [node.value for node in network[-1]]\n",
    "    print('Prediction =', preds)\n",
    "    devs = [abs(preds[p] - correct_out[t][p]) for p in range(len(preds))]\n",
    "    for d in devs:\n",
    "        assert d <= 0.02, 'Your prediction diverged too much from the true output!'\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: red;\">Task:</h2>\n",
    "\n",
    "There are many types of activation functions that are used by DNNs. Implement the following set of activation functions and their derivatives. **Do not use the trigonometric functions from the `math` module in your implementation!** Their mathematical definitions are given as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{tanh} \\left( z \\right) &= \\frac{2}{1 + e^{-2z}} - 1 &\n",
    "    \\frac{d}{dz} \\text{tanh} &= 1 - \\text{tanh} \\left( z \\right)^2 \\\\\n",
    "    \\text{ReLU} \\left( z \\right) &=\n",
    "    \\begin{cases}\n",
    "        z & \\text{if } z > 0 \\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{cases} &\n",
    "    \\frac{d}{dz} \\text{ReLU} &= 0.5 + 0.5 \\times \\text{sgn} \\left( z \\right) \\\\\n",
    "    \\text{LeakyReLU} \\left( z \\right) &=\n",
    "    \\begin{cases}\n",
    "        z & \\text{if } z > 0 \\\\\n",
    "        0.01z & \\text{otherwise}\n",
    "    \\end{cases} &\n",
    "    \\frac{d}{dz} \\text{LeakyReLU} &= 0.505 + 0.495 \\times \\text{sgn} \\left( z \\right) \\\\\n",
    "    \\text{SmoothReLU} \\left( z \\right) &= \\log \\left( 1 + e^z \\right) &\n",
    "    \\frac{d}{dz} \\text{SmoothReLU} &= \\frac{1}{1 + e^{-z}} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For those unfamiliar, $\\text{sgn}\\left(z\\right)$ refers to the sign function, which is defined as:\n",
    "\n",
    "$$\n",
    "\\text{sgn} \\left( z \\right) =\n",
    "\\begin{cases}\n",
    "    1 & \\text{if } z > 0 \\\\\n",
    "    0 & \\text{else if } z = 0 \\\\\n",
    "    -1 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "*Side note:* strictly speaking, the derivates for ReLU and LeakyReLU are undefined for 0. To avoid variations in how to handle this special case, we went ahead and defined the derivates as being the midpoint of the lower and upper values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hypertangent function\n",
    "\n",
    "Inputs:\n",
    "  z - input of the tanh function\n",
    "\"\"\"\n",
    "def tanh(z):\n",
    "    return 2 / (1 + math.exp(-2*z)) - 1\n",
    "\n",
    "\"\"\"\n",
    "The derivative of hypertangent function\n",
    "\n",
    "Inputs:\n",
    "  z - input of the tanh derivative function\n",
    "\"\"\"\n",
    "def tanh_deriv(z):\n",
    "    return 1 - tanh(z) ** 2\n",
    "\n",
    "\"\"\"\n",
    "Rectified Linear Unit (RELU)\n",
    "\n",
    "Inputs:\n",
    "  z - input of ReLU\n",
    "\"\"\"\n",
    "def relu(z):\n",
    "    if z > 0:\n",
    "        return z\n",
    "    return 0\n",
    "\n",
    "\"\"\"\n",
    "The derivative of RELU\n",
    "\n",
    "Inputs:\n",
    "  z - input of the RELU derivative function\n",
    "\"\"\"\n",
    "def relu_deriv(z):\n",
    "    if z > 0:\n",
    "        return 1\n",
    "    if z == 0:\n",
    "        return 0.5\n",
    "    return 0\n",
    "\n",
    "\"\"\"\n",
    "Leaky Rectified Linear Unit (leaky RELU)\n",
    "\n",
    "Inputs:\n",
    "  z - input of leaky RELU\n",
    "\"\"\"\n",
    "def leaky_relu(z):\n",
    "    if z > 0:\n",
    "        return z\n",
    "    return 0.01 * z\n",
    "\n",
    "\"\"\"\n",
    "The derivative of leaky RELU\n",
    "\n",
    "Inputs:\n",
    "  z - input of leaky RELU derivative function\n",
    "\"\"\"\n",
    "def leaky_relu_deriv(z):\n",
    "    if z > 0:\n",
    "        return 1\n",
    "    if z == 0:\n",
    "        return 0.505\n",
    "    return 0.01\n",
    "\n",
    "\"\"\"\n",
    "Smooth Rectified Linear Unit (smooth RELU)\n",
    "\n",
    "Inputs:\n",
    "  z - input of smooth RELU\n",
    "\"\"\"\n",
    "def smooth_relu(z):\n",
    "    return math.log(1 + math.exp(z))\n",
    "\n",
    "\"\"\"\n",
    "The derivative of smooth RELU\n",
    "\n",
    "Inputs:\n",
    "  z - input of leaky RELU derivative function\n",
    "\"\"\"\n",
    "def smooth_relu_deriv(z):\n",
    "    return 1 / (1 + math.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: red;\">Task:</h2>\n",
    "\n",
    "Now, let's compare the performance of the activation functions. To do this, we will use a different function for our DNN to approximate. Play around with the configurable variable values below and see how these **hyperparameters** (meaning parameters for the DNN hypothesis class) affect the performance for each activation function. **Which activation function(s) seem to work better for this problem?** Enter your response in the raw cell below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hypertangent and Leaky ReLU seem to work better for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic error rate: 43.75 %\n",
      "Hypertangent error rate: 0.0 %\n",
      "ReLU error rate: 31.25 %\n",
      "Leaky ReLU error rate: 0.0 %\n",
      "Smooth ReLU error rate: 6.25 %\n"
     ]
    }
   ],
   "source": [
    "# configurable variables\n",
    "# play around with these values and see how it affects the error rate\n",
    "learning_rate = 0.1\n",
    "num_epochs = 1000\n",
    "error_bound = 0.1\n",
    "\n",
    "# code to test performance\n",
    "# do not modify\n",
    "data_set = [([p,q,r,s],[int(p and q), int(q or r), int(r <= s), int(s != p)]) for p in [0,1] for q in [0,1] for r in [0,1] for s in [0,1]]\n",
    "random.shuffle(data_set)\n",
    "training_set = data_set[0:12]\n",
    "logistic_deriv = lambda z : logistic(z) * (1 - logistic(z))\n",
    "func_names = [\"Logistic\", \"Hypertangent\", \"ReLU\", \"Leaky ReLU\", \"Smooth ReLU\"]\n",
    "act_funcs = [logistic, tanh, relu, leaky_relu, smooth_relu]\n",
    "deriv_funcs = [logistic_deriv, tanh_deriv, relu_deriv, leaky_relu_deriv, smooth_relu_deriv]\n",
    "r = random.getstate()\n",
    "for (n, a, d) in zip(func_names, act_funcs, deriv_funcs):\n",
    "    random.setstate(r)\n",
    "    network = create_DNN(4, [8,8], 4, a, d)\n",
    "    back_prop(network, data_set, learning_rate, num_epochs)\n",
    "    test_set = [x for (x,y) in data_set[12:]]\n",
    "    correct_out = [y for (x,y) in data_set[12:]]\n",
    "    num_misses = 0\n",
    "    for t in range(len(test_set)):\n",
    "        forward_prop(network, test_set[t])\n",
    "        preds = [unit.value for unit in network[-1]]\n",
    "        devs = [abs(preds[p] - correct_out[t][p]) for p in range(len(preds))]\n",
    "        for d in devs:\n",
    "            if d > error_bound:\n",
    "                num_misses += 1\n",
    "    print(n, 'error rate:', num_misses / 16.0 * 100.0, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Naïve Bayes\n",
    "\n",
    "<h3 style=\"text-align: right; margin-top: -1em;\">30 points</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn our attention to the matter of probabilities. Part 1 mentioned how the logistic function has the nice feature of returning outputs that can be interpreted as probabilities. Let's continue with this train of thought. For this section, we will use $i$ to index over the $N$ samples in the data set, while $j$ will be used to index over the $n$ features in a particular sample $\\overline{x}_i$.\n",
    "\n",
    "In a supervised learning setting, we are interested in estimating $P\\left(Y \\mid X\\right)$, since we are given the data and want to predict the label. This is difficult to estimate, however, in a high dimensional setting. We could rely on Bayes' rule to help, but we end up with another issue when estimating $P\\left(X \\mid Y\\right)$. To make some progress, we take a huge leap of faith by making the **naïve Bayes assumption**, which states that the $n$ features in a sample are independent given the label: $P\\left(\\overline{x} \\mid y\\right) = \\prod_{j=1}^{n}P\\left(x_j \\mid y\\right)$.\n",
    "\n",
    "Taking all of this into account, we end up with the following **naïve Bayes classifier**:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h\\left(\\overline{x}\\right) &= \\underset{y}{\\text{argmax }} P\\left(y \\mid \\overline{x}\\right) \\\\\n",
    "&= \\underset{y}{\\text{argmax }} \\frac{P\\left(\\overline{x} \\mid y\\right) P\\left(y\\right)}{P\\left(\\overline{x}\\right)} & \\text{Bayes' rule} \\\\\n",
    "&= \\underset{y}{\\text{argmax }} P\\left(\\overline{x} \\mid y\\right) P\\left(y\\right) & P\\left(\\overline{x}\\right) \\text{does not depend on } y \\\\\n",
    "&= \\underset{y}{\\text{argmax }} P\\left(y\\right) \\prod_{j=1}^{n} P\\left(x_j \\mid y\\right) & \\text{naïve Bayes assumption} \\\\\n",
    "&= \\underset{y}{\\text{argmax }} \\log\\left(P\\left(y\\right)\\right) + \\sum_{j=1}^{n} \\log\\left(P\\left(x_j \\mid y\\right)\\right) & \\log\\text{is monotonic}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Estimating $P\\left(y\\right)$ is usually straightforward. We can estimate the probability of the label $y$ having value $c$ with the following (where $I\\left(\\cdot\\right)$ is the indicator function, also defined below):\n",
    "\n",
    "$$\n",
    "P\\left(y=c\\right) = \\frac{1}{N} \\sum_{i=1}^{N} I\\left(y_i=c\\right) \\\\\n",
    "I\\left(x\\right) =\n",
    "\\begin{cases}\n",
    "    1 & \\text{if } x \\text{ is true} \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "How about estimating $P\\left(x_j \\mid y\\right)$? This highly depends on what kind of features the data set contains. For this assignment, we will assume that we are dealing with **multinomial features**, meaning the features in the data represent counts. Thus, we have for any specific sample $\\overline{x}_i$:\n",
    "\n",
    "$$\n",
    "x_{ij} \\in \\{0,1,2,\\dots\\} \\text{ and } m_i = \\sum_{j=1}^{n} x_{ij}\n",
    "$$\n",
    "\n",
    "Note how $m_i$ can vary between samples, since not every sample's features will add up to the same sum. Under this assumption, we can precisely calculate the probability of a sample $\\overline{x}$ having the label value $c$ as:\n",
    "\n",
    "$$\n",
    "P\\left(\\overline{x}\\mid y=c\\right) = \\frac{\\left(\\sum_{j=1}^{n}x_j\\right)!}{\\prod_{j=1}^{n}x_j!} \\prod_{j=1}^{n} \\left(\\theta_{jc}\\right)^{x_j}\n",
    "$$\n",
    "\n",
    "We use $\\theta_{jc}$ as a parameter to refer to the probability of selecting feature $x_j$ and specify that $\\sum_{j=1}^{n}\\theta_{jc}=1$. While the above formula might seem intimidating, it does simplify when considered in the argmax setting of our classifier. This is because we can safely ignore the fraction as it does not depend on the label value $c$. We can estimate the $\\theta_{jc}$ parameter using the following equation (where $k$ is a smoothing parameter):\n",
    "\n",
    "$$\n",
    "\\theta_{jc} = \\frac{\\sum_{i=1}^{N} I\\left(y_i=c\\right)x_{ij}+k}{\\sum_{i=1}^{N} I\\left(y_i=c\\right)m_i+k\\times n}\n",
    "$$\n",
    "\n",
    "Finally, combining all of this together, we have that:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\underset{c}{\\text{argmax }}P\\left(y=c\\mid\\overline{x}\\right)\n",
    "&\\propto\\underset{c}{\\text{argmax }}P\\left(y=c\\right)\\prod_{j=1}^{n}\\left(\\theta_{jc}\\right)^{x_j} \\\\\n",
    "&\\propto\\underset{c}{\\text{argmax }}\\log\\left(P\\left(y=c\\right)\\right)+\\sum_{j=1}^{n}\\log\\left(\\left(\\theta_{jc}\\right)^{x_j}\\right) \\\\\n",
    "&\\propto\\underset{c}{\\text{argmax }}\\log\\left(P\\left(y=c\\right)\\right)+\\sum_{j=1}^{n}x_j\\log\\left(\\theta_{jc}\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore, for the naïve Bayes classifier (under a multinomial distribution) to make a prediction for a sample $\\overline{x}$, it would iterate over all label values to find the value $c$ that maximizes the above formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: red;\">Task:</h2>\n",
    "\n",
    "Implement the naïve Bayes classifier, `naive_bayes`, and its helper functions below. `label_est` should estimate $P\\left(y=c\\right)$. `theta_est` should estimate $\\theta_{jc}$. You may assume the label space $\\mathcal{Y}$ is binary, i.e., $y\\in\\{0,1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Estimate P(y = c)\n",
    "\n",
    "Inputs:\n",
    "  data_set - data set, a list of data points (x, y), where x is a vector of size n, y is either 0 or 1\n",
    "  target_label - target label, either 0 or 1\n",
    "\"\"\"\n",
    "def label_est(data_set, target_label):\n",
    "    res = 0\n",
    "    for x, y in data_set:\n",
    "        res += int(y == target_label) / len(data_set) \n",
    "    return res\n",
    "\n",
    "\"\"\"\n",
    "Estimate theata_jc\n",
    "\n",
    "Inputs:\n",
    "  data_set - data set, a list of data points (x, y), where x is a vector of size n, y is either 0 or 1\n",
    "  target_label - target label, either 0 or 1\n",
    "  j - feature\n",
    "  smooth - smooth parameter\n",
    "\"\"\"\n",
    "def theta_est(data_set, target_label, j, smooth):\n",
    "    num, den = 0, 0\n",
    "    for x, y in data_set:\n",
    "        num += int(y == target_label) * x[j]\n",
    "        den += int(y == target_label) * sum(x)\n",
    "    return (num + smooth) / (den + smooth * len(x))\n",
    "\n",
    "\"\"\"\n",
    "Naive Bayes classifier\n",
    "\n",
    "Inputs: \n",
    "  data_set - data set, a list of data points (x, y), where x is a vector of size n, y is either 0 or 1\n",
    "  x - test data point (sample)\n",
    "  smooth - smooth parameter\n",
    "\"\"\"\n",
    "def naive_bayes(data_set, x, smooth):\n",
    "    first_0 = math.log(label_est(data_set, 0))\n",
    "    first_1 = math.log(label_est(data_set, 1))\n",
    "    second_0, second_1 = 0, 0\n",
    "    for j in range(len(x)):\n",
    "        second_0 += x[j] * math.log(theta_est(data_set, 0, j, smooth))\n",
    "        second_1 += x[j] * math.log(theta_est(data_set, 1, j, smooth))\n",
    "    if first_0 + second_0 > first_1 + second_1:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: red;\">Task:</h2>\n",
    "\n",
    "Now let's test your implementation. The cell below has a pre-defined data set and test sample. At the moment, the smoothing parameter is set to 0 for the naïve Bayes classifier. Run the cell as-is and observe what the classifier predicts. Then, change the smoothing parameter to 1 and run the cell again. **What happens to the prediction? Why does this occur? What if we change the smoothing parameter to some integer greater than 1?** Give your answers in the raw cell below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If the hyperparameter is set 0, the prediction is 0 (wrong prediction). If we change the hyperparameter to 1, then the prediction changes to 1 (correct prediction). This is probably because we need some kind of 'fail-safe' probability (regularization). The worst case is that none of the data in the training set appear in the test set, which is exactly our case in this problem. Then the posterior probability will be 0 and the test sample will be very likely to be predicted wrong. If we change the hyperparameter to some integer greater than 1, the prediction is still correct (1). This is probably because larger hyperparameter is still good for this case. However generally it can't be very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sample: [9, 3]\n",
      "True label: 1\n",
      "Naive Bayes prediction (with smoothing 1): 1\n"
     ]
    }
   ],
   "source": [
    "data_set = [([4, 5], 1), ([4, 0], 0), ([3, 7], 1), ([0, 8], 1), ([2, 2], 0), ([1, 3], 0), ([6, 6], 1), ([9, 5], 1), ([1, 5], 1), ([3, 2], 1), ([5, 0], 1), ([4, 6], 1), ([0, 5], 1), ([0, 9], 1), ([2, 6], 1), ([1, 6], 1), ([4, 3], 1), ([6, 3], 1), ([8, 9], 1), ([2, 5], 1), ([1, 2], 0), ([3, 9], 1), ([5, 3], 1), ([3, 5], 1), ([8, 4], 1)]\n",
    "test_sample = ([9, 3], 1)\n",
    "smooth = 1\n",
    "pred = naive_bayes(data_set, test_sample[0], smooth)\n",
    "print('Test sample:', test_sample[0])\n",
    "print('True label:', test_sample[1])\n",
    "print('Naive Bayes prediction (with smoothing {}): {}'.format(smooth, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Just for Fun\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of points is said to be **shattered** by the set of all linear classifiers if, for all ways of assigning labels to the points, there exists a linear classifier that will give the points the assigned labels. Thus, for example, any set of three points in $\\mathbb{R}^2$ that are not colinear can be shattered by linear classifiers (see the figure below). However, all sets of four points (such as the four points from the XOR example) cannot be shattered. **What is the largest set of points that can be shattered for points in $\\mathbb{R}^3$**? Provide your answer in the raw text cell below.\n",
    "\n",
    "![](shattered_2d.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will only be submitting your Jupyter Notebook file, *hw5.ipynb*. Do not worry about submitting the additional files that came with the assignment. Furthermore, as a reminder, part of your grade is your documentation. Each of the functions you implemented as part of this assignment **must** be documented; failure to do so will result in a penalty.\n",
    "\n",
    "Please upload your *hw5.ipynb* file to CMS by **Wednesday, April 17 @ 1:24pm**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
